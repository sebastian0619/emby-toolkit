# core_processor_sa.py

import os
import json
import sqlite3
import concurrent.futures
from typing import Dict, List, Optional, Any, Tuple
import shutil
import threading
import time
import requests
import copy
import random
# ç¡®ä¿æ‰€æœ‰ä¾èµ–éƒ½å·²æ­£ç¡®å¯¼å…¥
import emby_handler
import tmdb_handler
import utils
import constants
import logging
import actor_utils
from cachetools import TTLCache
from db_handler import ActorDBManager
from db_handler import get_db_connection as get_central_db_connection
from ai_translator import AITranslator
from utils import LogDBManager, get_override_path_for_item, translate_country_list
from watchlist_processor import WatchlistProcessor
from douban import DoubanApi

logger = logging.getLogger(__name__)
try:
    from douban import DoubanApi
    DOUBAN_API_AVAILABLE = True
except ImportError:
    DOUBAN_API_AVAILABLE = False
    class DoubanApi:
        def __init__(self, *args, **kwargs): pass
        def get_acting(self, *args, **kwargs): return {}
        def close(self): pass

def _read_local_json(file_path: str) -> Optional[Dict[str, Any]]:
    if not os.path.exists(file_path):
        logger.warning(f"æœ¬åœ°å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return None
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"è¯»å–æœ¬åœ°JSONæ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}")
        return None
def _save_metadata_to_cache(
    cursor: sqlite3.Cursor,
    tmdb_id: str,
    item_type: str,
    item_details_from_emby: Dict[str, Any], # â˜… Emby çš„è¯¦æƒ…
    final_processed_cast: List[Dict[str, Any]], # â˜… æˆ‘ä»¬æœ€ç»ˆå¤„ç†å¥½çš„æ¼”å‘˜è¡¨
    tmdb_details_for_extra: Optional[Dict[str, Any]] # â˜… å¯é€‰çš„ã€ç”¨äºè¡¥å……å¯¼æ¼”å’Œå›½å®¶çš„ TMDB è¯¦æƒ…
):
    """
    ã€V-API-Native - APIåŸç”Ÿç‰ˆã€‘
    ç›´æ¥ä»å¤„ç†å¥½çš„æ•°æ®ï¼ˆEmbyè¯¦æƒ…ã€æœ€ç»ˆæ¼”å‘˜è¡¨ã€å¯é€‰çš„TMDBè¯¦æƒ…ï¼‰ç»„è£…å¹¶ç¼“å­˜å…ƒæ•°æ®ã€‚
    """
    try:
        logger.debug(f"ã€å®æ—¶ç¼“å­˜-APIåŸç”Ÿç‰ˆã€‘æ­£åœ¨ä¸º '{item_details_from_emby.get('Name')}' ç»„è£…å…ƒæ•°æ®...")
        
        # --- ä»æˆ‘ä»¬å·²æœ‰çš„ã€æœ€å¯é çš„æ•°æ®æºä¸­ç»„è£…æ‰€æœ‰ä¿¡æ¯ ---
        
        # 1. æ¼”å‘˜ (æ¥è‡ªæˆ‘ä»¬æœ€ç»ˆå¤„ç†å¥½çš„æ¼”å‘˜è¡¨)
        actors = [
            {"id": p.get("id"), "name": p.get("name"), "original_name": p.get("original_name")}
            for p in final_processed_cast
        ]

        # 2. å¯¼æ¼”å’Œå›½å®¶ (ä¼˜å…ˆä»è¡¥å……çš„ TMDB è¯¦æƒ…ä¸­è·å–)
        directors, countries = [], []
        if tmdb_details_for_extra:
            if item_type == 'Movie':
                credits_data = tmdb_details_for_extra.get("credits", {}) or tmdb_details_for_extra.get("casts", {})
                if credits_data:
                    directors = [{'id': p.get('id'), 'name': p.get('name')} for p in credits_data.get('crew', []) if p.get('job') == 'Director']
                country_names = [c['name'] for c in tmdb_details_for_extra.get('production_countries', [])]
                countries = translate_country_list(country_names)
            elif item_type == 'Series':
                credits_data = tmdb_details_for_extra.get("credits", {})
                if credits_data:
                    directors = [{'id': p.get('id'), 'name': p.get('name')} for p in credits_data.get('crew', []) if p.get('job') == 'Director']
                if not directors:
                    directors = [{'id': c.get('id'), 'name': c.get('name')} for c in tmdb_details_for_extra.get('created_by', [])]
                country_codes = tmdb_details_for_extra.get('origin_country', [])
                countries = translate_country_list(country_codes)
        
        # 3. å…¶ä»–ä¿¡æ¯ (ä¸»è¦ä» Emby è¯¦æƒ…ä¸­è·å–ï¼Œå› ä¸ºè¿™æ˜¯æœ€å®æ—¶çš„)
        studios = [s['Name'] for s in item_details_from_emby.get('Studios', [])]
        genres = item_details_from_emby.get('Genres', [])
        release_date_str = (item_details_from_emby.get('PremiereDate') or '0000-01-01T00:00:00.000Z').split('T')[0]
        
        # --- å‡†å¤‡è¦å­˜å…¥æ•°æ®åº“çš„æ•°æ® ---
        metadata = {
            "tmdb_id": tmdb_id,
            "item_type": item_type,
            "title": item_details_from_emby.get('Name'),
            "original_title": item_details_from_emby.get('OriginalTitle'),
            "release_year": item_details_from_emby.get('ProductionYear'),
            "rating": item_details_from_emby.get('CommunityRating'),
            "genres_json": json.dumps(genres, ensure_ascii=False),
            "actors_json": json.dumps(actors, ensure_ascii=False),
            "directors_json": json.dumps(directors, ensure_ascii=False),
            "studios_json": json.dumps(studios, ensure_ascii=False),
            "countries_json": json.dumps(countries, ensure_ascii=False),
            "date_added": (item_details_from_emby.get("DateCreated") or '').split('T')[0] or None,
            "release_date": release_date_str,
        }
        
        # --- æ•°æ®åº“å†™å…¥ ---
        columns = ', '.join(metadata.keys())
        placeholders = ', '.join('?' for _ in metadata)
        sql = f"INSERT OR REPLACE INTO media_metadata ({columns}) VALUES ({placeholders})"
        cursor.execute(sql, tuple(metadata.values()))
        logger.debug(f"  -> æˆåŠŸå°†ã€Š{metadata.get('title')}ã€‹çš„å…ƒæ•°æ®ç¼“å­˜åˆ°æ•°æ®åº“ã€‚")

    except Exception as e:
        logger.error(f"ä¿å­˜å…ƒæ•°æ®åˆ°ç¼“å­˜è¡¨æ—¶å¤±è´¥: {e}", exc_info=True)
def _aggregate_series_cast_from_tmdb_data(series_data: Dict[str, Any], all_episodes_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    ã€æ–°ã€‘ä»å†…å­˜ä¸­çš„TMDBæ•°æ®èšåˆä¸€ä¸ªå‰§é›†çš„æ‰€æœ‰æ¼”å‘˜ã€‚
    """
    logger.debug(f"ã€æ¼”å‘˜èšåˆã€‘å¼€å§‹ä¸º '{series_data.get('name')}' ä»å†…å­˜ä¸­çš„TMDBæ•°æ®èšåˆæ¼”å‘˜...")
    aggregated_cast_map = {}

    # 1. ä¼˜å…ˆå¤„ç†ä¸»å‰§é›†çš„æ¼”å‘˜åˆ—è¡¨
    main_cast = series_data.get("credits", {}).get("cast", [])
    for actor in main_cast:
        actor_id = actor.get("id")
        if actor_id:
            aggregated_cast_map[actor_id] = actor
    logger.debug(f"  -> ä»ä¸»å‰§é›†æ•°æ®ä¸­åŠ è½½äº† {len(aggregated_cast_map)} ä½ä¸»æ¼”å‘˜ã€‚")

    # 2. èšåˆæ‰€æœ‰åˆ†é›†çš„æ¼”å‘˜å’Œå®¢ä¸²æ¼”å‘˜
    for episode_data in all_episodes_data:
        credits_data = episode_data.get("credits", {})
        actors_to_process = credits_data.get("cast", []) + credits_data.get("guest_stars", [])
        
        for actor in actors_to_process:
            actor_id = actor.get("id")
            if actor_id and actor_id not in aggregated_cast_map:
                if 'order' not in actor:
                    actor['order'] = 999  # ä¸ºå®¢ä¸²æ¼”å‘˜è®¾ç½®é«˜orderå€¼
                aggregated_cast_map[actor_id] = actor

    full_aggregated_cast = list(aggregated_cast_map.values())
    full_aggregated_cast.sort(key=lambda x: x.get('order', 999))
    
    logger.info(f"ã€æ¼”å‘˜èšåˆã€‘å®Œæˆã€‚å…±ä¸º '{series_data.get('name')}' èšåˆäº† {len(full_aggregated_cast)} ä½ç‹¬ç«‹æ¼”å‘˜ã€‚")
    return full_aggregated_cast
class MediaProcessor:
    def __init__(self, config: Dict[str, Any]):
        # â˜…â˜…â˜… ç„¶åï¼Œä»è¿™ä¸ª config å­—å…¸é‡Œï¼Œè§£æå‡ºæ‰€æœ‰éœ€è¦çš„å±æ€§ â˜…â˜…â˜…
        self.config = config
        self.db_path = config.get('db_path')
        if not self.db_path:
            raise ValueError("æ•°æ®åº“è·¯å¾„ (db_path) æœªåœ¨é…ç½®ä¸­æä¾›ã€‚")

        # åˆå§‹åŒ–æˆ‘ä»¬çš„æ•°æ®åº“ç®¡ç†å‘˜
        self.actor_db_manager = ActorDBManager(self.db_path)
        self.log_db_manager = LogDBManager(self.db_path)

        # ä» config ä¸­è·å–æ‰€æœ‰å…¶ä»–é…ç½®
        self.douban_api = None
        if getattr(constants, 'DOUBAN_API_AVAILABLE', False):
            try:
                # --- âœ¨âœ¨âœ¨ æ ¸å¿ƒä¿®æ”¹åŒºåŸŸ START âœ¨âœ¨âœ¨ ---

                # 1. ä»é…ç½®ä¸­è·å–å†·å´æ—¶é—´ 
                douban_cooldown = self.config.get(constants.CONFIG_OPTION_DOUBAN_DEFAULT_COOLDOWN, 2.0)
                
                # 2. ä»é…ç½®ä¸­è·å– Cookieï¼Œä½¿ç”¨æˆ‘ä»¬åˆšåˆšåœ¨ constants.py ä¸­å®šä¹‰çš„å¸¸é‡
                douban_cookie = self.config.get(constants.CONFIG_OPTION_DOUBAN_COOKIE, "")
                
                # 3. æ·»åŠ ä¸€ä¸ªæ—¥å¿—ï¼Œæ–¹ä¾¿è°ƒè¯•
                if not douban_cookie:
                    logger.debug(f"é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°æˆ–æœªè®¾ç½® '{constants.CONFIG_OPTION_DOUBAN_COOKIE}'ã€‚å¦‚æœè±†ç“£APIè¿”å›'need_login'é”™è¯¯ï¼Œè¯·é…ç½®è±†ç“£cookieã€‚")
                else:
                    logger.debug("å·²ä»é…ç½®ä¸­åŠ è½½è±†ç“£ Cookieã€‚")

                # 4. å°†æ‰€æœ‰å‚æ•°ä¼ é€’ç»™ DoubanApi çš„æ„é€ å‡½æ•°
                self.douban_api = DoubanApi(
                    cooldown_seconds=douban_cooldown,
                    user_cookie=douban_cookie  # <--- å°† cookie ä¼ è¿›å»
                )
                logger.trace("DoubanApi å®ä¾‹å·²åœ¨ MediaProcessorAPI ä¸­åˆ›å»ºã€‚")
                
                # --- âœ¨âœ¨âœ¨ æ ¸å¿ƒä¿®æ”¹åŒºåŸŸ END âœ¨âœ¨âœ¨ ---

            except Exception as e:
                logger.error(f"MediaProcessorAPI åˆå§‹åŒ– DoubanApi å¤±è´¥: {e}", exc_info=True)
        else:
            logger.warning("DoubanApi å¸¸é‡æŒ‡ç¤ºä¸å¯ç”¨ï¼Œå°†ä¸ä½¿ç”¨è±†ç“£åŠŸèƒ½ã€‚")
        self.emby_url = self.config.get("emby_server_url")
        self.emby_api_key = self.config.get("emby_api_key")
        self.emby_user_id = self.config.get("emby_user_id")
        self.tmdb_api_key = self.config.get("tmdb_api_key", "")
        self.local_data_path = self.config.get("local_data_path", "").strip()
        self.auto_lock_cast_enabled = self.config.get(constants.CONFIG_OPTION_AUTO_LOCK_CAST, True)
        
        self.ai_enabled = self.config.get("ai_translation_enabled", False)
        self.ai_translator = AITranslator(self.config) if self.ai_enabled else None
        
        self._stop_event = threading.Event()
        self.processed_items_cache = self._load_processed_log_from_db()
        self.manual_edit_cache = TTLCache(maxsize=10, ttl=600)
        logger.trace("æ ¸å¿ƒå¤„ç†å™¨åˆå§‹åŒ–å®Œæˆã€‚")
    # --- æ¸…é™¤å·²å¤„ç†è®°å½• ---
    def clear_processed_log(self):
        """
        ã€å·²æ”¹é€ ã€‘æ¸…é™¤æ•°æ®åº“å’Œå†…å­˜ä¸­çš„å·²å¤„ç†è®°å½•ã€‚
        ä½¿ç”¨ä¸­å¤®æ•°æ®åº“è¿æ¥å‡½æ•°ã€‚
        """
        try:
            # 1. â˜…â˜…â˜… è°ƒç”¨ä¸­å¤®å‡½æ•°ï¼Œå¹¶ä¼ å…¥ self.db_path â˜…â˜…â˜…
            with get_central_db_connection(self.db_path) as conn:
                cursor = conn.cursor()
                
                logger.debug("æ­£åœ¨ä»æ•°æ®åº“åˆ é™¤ processed_log è¡¨ä¸­çš„æ‰€æœ‰è®°å½•...")
                cursor.execute("DELETE FROM processed_log")
                # with è¯­å¥ä¼šè‡ªåŠ¨å¤„ç† conn.commit()
            
            logger.info("æ•°æ®åº“ä¸­çš„å·²å¤„ç†è®°å½•å·²æ¸…é™¤ã€‚")

            # 2. æ¸…ç©ºå†…å­˜ç¼“å­˜
            self.processed_items_cache.clear()
            logger.info("å†…å­˜ä¸­çš„å·²å¤„ç†è®°å½•ç¼“å­˜å·²æ¸…é™¤ã€‚")

        except Exception as e:
            logger.error(f"æ¸…é™¤æ•°æ®åº“æˆ–å†…å­˜å·²å¤„ç†è®°å½•æ—¶å¤±è´¥: {e}", exc_info=True)
            # 3. â˜…â˜…â˜… é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œé€šçŸ¥ä¸Šæ¸¸è°ƒç”¨è€…æ“ä½œå¤±è´¥ â˜…â˜…â˜…
            raise
    # â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜… æ–°å¢çš„ã€ä¼˜é›…çš„å†…éƒ¨è¾…åŠ©æ–¹æ³• â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…
    def _enrich_cast_from_db_and_api(self, cast_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        åœ¨å†…éƒ¨å¤„ç† sqlite3.Rowï¼Œä½†å¯¹å¤–è¿”å›æ ‡å‡†çš„ dict åˆ—è¡¨ï¼Œç¡®ä¿ä¸‹æ¸¸å…¼å®¹æ€§ã€‚
        """
        if not cast_list:
            return []
        
        logger.info(f"  -> å¤„ç† {len(cast_list)} ä½æ¼”å‘˜...")

        original_actor_map = {str(actor.get("Id")): actor for actor in cast_list if actor.get("Id")}
        
        # --- é˜¶æ®µä¸€ï¼šä»æœ¬åœ°æ•°æ®åº“è·å–æ•°æ® ---
        enriched_actors_map = {}
        ids_found_in_db = set()
        
        try:
            # â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜… å…³é”®ä¿®æ”¹ï¼šåœ¨è¿™é‡Œè·å–è¿æ¥å¹¶è®¾ç½® row_factory â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…
            with get_central_db_connection(self.db_path) as conn:
                # conn.row_factory = sqlite3.Row # å‡è®¾ get_central_db_connection å·²ç»è®¾ç½®äº†
                cursor = conn.cursor()
                person_ids = list(original_actor_map.keys())
                if person_ids:
                    placeholders = ','.join('?' for _ in person_ids)
                    query = f"SELECT * FROM person_identity_map WHERE emby_person_id IN ({placeholders})"
                    cursor.execute(query, person_ids)
                    db_results = cursor.fetchall()

                    for row in db_results:
                        # â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜… å…³é”®ä¿®æ”¹ï¼šç«‹å³å°† sqlite3.Row è½¬æ¢ä¸º dict â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…
                        db_data = dict(row)
                        
                        actor_id = str(db_data["emby_person_id"])
                        ids_found_in_db.add(actor_id)
                        
                        provider_ids = {}
                        # ç°åœ¨å¯ä»¥å®‰å…¨åœ°ä½¿ç”¨ .get() æ–¹æ³•äº†
                        if db_data.get("tmdb_person_id"): provider_ids["Tmdb"] = str(db_data.get("tmdb_person_id"))
                        if db_data.get("imdb_id"): provider_ids["Imdb"] = db_data.get("imdb_id")
                        if db_data.get("douban_celebrity_id"): provider_ids["Douban"] = str(db_data.get("douban_celebrity_id"))
                        
                        enriched_actor = original_actor_map[actor_id].copy()
                        enriched_actor["ProviderIds"] = provider_ids
                        enriched_actors_map[actor_id] = enriched_actor
        except Exception as e:
            logger.error(f"  -> æ•°æ®åº“æŸ¥è¯¢é˜¶æ®µå¤±è´¥: {e}", exc_info=True)

        logger.info(f"  -> é˜¶æ®µä¸€ (æ•°æ®åº“) å®Œæˆï¼šæ‰¾åˆ°äº† {len(ids_found_in_db)} ä½æ¼”å‘˜çš„ç¼“å­˜ä¿¡æ¯ã€‚")

        # --- é˜¶æ®µäºŒï¼šä¸ºæœªæ‰¾åˆ°çš„æ¼”å‘˜å®æ—¶æŸ¥è¯¢ Emby API (è¿™éƒ¨åˆ†é€»è¾‘ä¸å˜) ---
        ids_to_fetch_from_api = [pid for pid in original_actor_map.keys() if pid not in ids_found_in_db]
        
        if ids_to_fetch_from_api:
            logger.info(f"  -> é˜¶æ®µäºŒ (APIæŸ¥è¯¢) å¼€å§‹ï¼šä¸º {len(ids_to_fetch_from_api)} ä½æ–°æ¼”å‘˜å®æ—¶è·å–ä¿¡æ¯...")
            for i, actor_id in enumerate(ids_to_fetch_from_api):
                # ... (è¿™é‡Œçš„ API è°ƒç”¨é€»è¾‘ä¿æŒä¸å˜) ...
                full_detail = emby_handler.get_emby_item_details(
                    item_id=actor_id,
                    emby_server_url=self.emby_url,
                    emby_api_key=self.emby_api_key,
                    user_id=self.emby_user_id,
                    fields="ProviderIds,Name" # åªè¯·æ±‚æœ€å…³é”®çš„ä¿¡æ¯
                )
                if full_detail and full_detail.get("ProviderIds"):
                    enriched_actor = original_actor_map[actor_id].copy()
                    enriched_actor["ProviderIds"] = full_detail["ProviderIds"]
                    enriched_actors_map[actor_id] = enriched_actor
                else:
                    logger.warning(f"    æœªèƒ½ä» API è·å–åˆ°æ¼”å‘˜ ID {actor_id} çš„ ProviderIdsã€‚")
        else:
            logger.info("  -> é˜¶æ®µäºŒ (APIæŸ¥è¯¢) è·³è¿‡ï¼šæ‰€æœ‰æ¼”å‘˜å‡åœ¨æœ¬åœ°æ•°æ®åº“ä¸­æ‰¾åˆ°ã€‚")

        # --- é˜¶æ®µä¸‰ï¼šåˆå¹¶æœ€ç»ˆç»“æœ (è¿™éƒ¨åˆ†é€»è¾‘ä¸å˜) ---
        final_enriched_cast = []
        for original_actor in cast_list:
            actor_id = str(original_actor.get("Id"))
            final_enriched_cast.append(enriched_actors_map.get(actor_id, original_actor))

        return final_enriched_cast
    # â˜…â˜…â˜… å…¬å¼€çš„ã€ç‹¬ç«‹çš„è¿½å‰§åˆ¤æ–­æ–¹æ³• â˜…â˜…â˜…
    def check_and_add_to_watchlist(self, item_details: Dict[str, Any]):
        """
        æ£€æŸ¥ä¸€ä¸ªåª’ä½“é¡¹ç›®æ˜¯å¦ä¸ºå‰§é›†ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™æ‰§è¡Œæ™ºèƒ½è¿½å‰§åˆ¤æ–­å¹¶æ·»åŠ åˆ°å¾…çœ‹åˆ—è¡¨ã€‚
        æ­¤æ–¹æ³•è¢«è®¾è®¡ä¸ºç”±å¤–éƒ¨äº‹ä»¶ï¼ˆå¦‚Webhookï¼‰æ˜¾å¼è°ƒç”¨ã€‚
        """
        item_name_for_log = item_details.get("Name", f"æœªçŸ¥é¡¹ç›®(ID:{item_details.get('Id')})")
        
        if item_details.get("Type") != "Series":
            # å¦‚æœä¸æ˜¯å‰§é›†ï¼Œç›´æ¥è¿”å›ï¼Œä¸æ‰“å°éå¿…è¦çš„æ—¥å¿—
            return

        logger.info(f"Webhookè§¦å‘ï¼šå¼€å§‹ä¸ºæ–°å…¥åº“å‰§é›† '{item_name_for_log}' è¿›è¡Œè¿½å‰§çŠ¶æ€åˆ¤æ–­...")
        try:
            # å®ä¾‹åŒ– WatchlistProcessor å¹¶æ‰§è¡Œæ·»åŠ æ“ä½œ
            watchlist_proc = WatchlistProcessor(self.config)
            watchlist_proc.add_series_to_watchlist(item_details)
        except Exception as e_watchlist:
            logger.error(f"åœ¨è‡ªåŠ¨æ·»åŠ  '{item_name_for_log}' åˆ°è¿½å‰§åˆ—è¡¨æ—¶å‘ç”Ÿé”™è¯¯: {e_watchlist}", exc_info=True)

    def signal_stop(self):
        self._stop_event.set()

    def clear_stop_signal(self):
        self._stop_event.clear()

    def get_stop_event(self) -> threading.Event:
        """è¿”å›å†…éƒ¨çš„åœæ­¢äº‹ä»¶å¯¹è±¡ï¼Œä»¥ä¾¿ä¼ é€’ç»™å…¶ä»–å‡½æ•°ã€‚"""
        return self._stop_event

    def is_stop_requested(self) -> bool:
        return self._stop_event.is_set()

    def _load_processed_log_from_db(self) -> Dict[str, str]:
        log_dict = {}
        try:
            # 1. â˜…â˜…â˜… ä½¿ç”¨ with è¯­å¥å’Œä¸­å¤®å‡½æ•° â˜…â˜…â˜…
            with get_central_db_connection(self.db_path) as conn:
                cursor = conn.cursor()
                
                # 2. æ‰§è¡ŒæŸ¥è¯¢
                cursor.execute("SELECT item_id, item_name FROM processed_log")
                rows = cursor.fetchall()
                
                # 3. å¤„ç†ç»“æœ
                for row in rows:
                    if row['item_id'] and row['item_name']:
                        log_dict[row['item_id']] = row['item_name']
            
            # 4. with è¯­å¥ä¼šè‡ªåŠ¨å¤„ç†æ‰€æœ‰äº‹æƒ…ï¼Œä»£ç å¹²å‡€åˆ©è½ï¼

        except Exception as e:
            # 5. â˜…â˜…â˜… è®°å½•æ›´è¯¦ç»†çš„å¼‚å¸¸ä¿¡æ¯ â˜…â˜…â˜…
            logger.error(f"ä»æ•°æ®åº“è¯»å–å·²å¤„ç†è®°å½•å¤±è´¥: {e}", exc_info=True)
        return log_dict

    # âœ¨ ä» SyncHandler è¿ç§»å¹¶æ”¹é€ ï¼Œç”¨äºåœ¨æœ¬åœ°ç¼“å­˜ä¸­æŸ¥æ‰¾è±†ç“£JSONæ–‡ä»¶
    def _find_local_douban_json(self, imdb_id: Optional[str], douban_id: Optional[str], douban_cache_dir: str) -> Optional[str]:
        """æ ¹æ® IMDb ID æˆ– è±†ç“£ ID åœ¨æœ¬åœ°ç¼“å­˜ç›®å½•ä¸­æŸ¥æ‰¾å¯¹åº”çš„è±†ç“£JSONæ–‡ä»¶ã€‚"""
        if not os.path.exists(douban_cache_dir):
            return None
        
        # ä¼˜å…ˆä½¿ç”¨ IMDb ID åŒ¹é…ï¼Œæ›´å‡†ç¡®
        if imdb_id:
            for dirname in os.listdir(douban_cache_dir):
                if dirname.startswith('0_'): continue
                if imdb_id in dirname:
                    dir_path = os.path.join(douban_cache_dir, dirname)
                    for filename in os.listdir(dir_path):
                        if filename.endswith('.json'):
                            return os.path.join(dir_path, filename)
                            
        # å…¶æ¬¡ä½¿ç”¨è±†ç“£ ID åŒ¹é…
        if douban_id:
            for dirname in os.listdir(douban_cache_dir):
                if dirname.startswith(f"{douban_id}_"):
                    dir_path = os.path.join(douban_cache_dir, dirname)
                    for filename in os.listdir(dir_path):
                        if filename.endswith('.json'):
                            return os.path.join(dir_path, filename)
        return None

    # âœ¨ å°è£…äº†â€œä¼˜å…ˆæœ¬åœ°ç¼“å­˜ï¼Œå¤±è´¥åˆ™åœ¨çº¿è·å–â€çš„é€»è¾‘
    def _get_douban_data_with_local_cache(self, media_info: Dict[str, Any]) -> Tuple[List[Dict[str, Any]], Optional[float]]:
        """
        ã€V3 - æœ€ç»ˆç‰ˆã€‘è·å–è±†ç“£æ•°æ®ï¼ˆæ¼”å‘˜+è¯„åˆ†ï¼‰ã€‚ä¼˜å…ˆæœ¬åœ°ç¼“å­˜ï¼Œå¤±è´¥åˆ™å›é€€åˆ°åŠŸèƒ½å®Œæ•´çš„åœ¨çº¿APIè·¯å¾„ã€‚
        è¿”å›: (æ¼”å‘˜åˆ—è¡¨, è±†ç“£è¯„åˆ†) çš„å…ƒç»„ã€‚
        """
        # 1. å‡†å¤‡æŸ¥æ‰¾æ‰€éœ€çš„ä¿¡æ¯
        provider_ids = media_info.get("ProviderIds", {})
        item_name = media_info.get("Name", "")
        imdb_id = provider_ids.get("Imdb")
        douban_id_from_provider = provider_ids.get("Douban")
        item_type = media_info.get("Type")
        item_year = str(media_info.get("ProductionYear", ""))

        # 2. å°è¯•ä»æœ¬åœ°ç¼“å­˜æŸ¥æ‰¾
        douban_cache_dir_name = "douban-movies" if item_type == "Movie" else "douban-tv"
        douban_cache_path = os.path.join(self.local_data_path, "cache", douban_cache_dir_name)
        local_json_path = self._find_local_douban_json(imdb_id, douban_id_from_provider, douban_cache_path)

        if local_json_path:
            logger.debug(f"å‘ç°æœ¬åœ°è±†ç“£ç¼“å­˜æ–‡ä»¶ï¼Œå°†ç›´æ¥ä½¿ç”¨: {local_json_path}")
            douban_data = _read_local_json(local_json_path)
            if douban_data:
                cast = douban_data.get('actors', [])
                rating_str = douban_data.get("rating", {}).get("value")
                rating_float = None
                if rating_str:
                    try: rating_float = float(rating_str)
                    except (ValueError, TypeError): pass
                return cast, rating_float
            else:
                logger.warning(f"æœ¬åœ°è±†ç“£ç¼“å­˜æ–‡ä»¶ '{local_json_path}' æ— æ•ˆï¼Œå°†å›é€€åˆ°åœ¨çº¿APIã€‚")
        
        # 3. å¦‚æœæœ¬åœ°æœªæ‰¾åˆ°ï¼Œå›é€€åˆ°åŠŸèƒ½å®Œæ•´çš„åœ¨çº¿APIè·¯å¾„
        logger.info("æœªæ‰¾åˆ°æœ¬åœ°è±†ç“£ç¼“å­˜ï¼Œå°†é€šè¿‡åœ¨çº¿APIè·å–æ¼”å‘˜å’Œè¯„åˆ†ä¿¡æ¯ã€‚")

        # 3.1 åŒ¹é…è±†ç“£IDå’Œç±»å‹ã€‚ç°åœ¨ match_info è¿”å›çš„ç»“æœæ˜¯å®Œå…¨å¯ä¿¡çš„ã€‚
        match_info_result = self.douban_api.match_info(
            name=item_name, imdbid=imdb_id, mtype=item_type, year=item_year
        )

        if match_info_result.get("error") or not match_info_result.get("id"):
            logger.warning(f"åœ¨çº¿åŒ¹é…è±†ç“£IDå¤±è´¥ for '{item_name}': {match_info_result.get('message', 'æœªæ‰¾åˆ°ID')}")
            return [], None

        douban_id = match_info_result["id"]
        # âœ¨âœ¨âœ¨ ç›´æ¥ä¿¡ä»»ä» douban.py è¿”å›çš„ç±»å‹ âœ¨âœ¨âœ¨
        douban_type = match_info_result.get("type")

        if not douban_type:
            logger.error(f"ä»è±†ç“£åŒ¹é…ç»“æœä¸­æœªèƒ½è·å–åˆ°åª’ä½“ç±»å‹ for ID {douban_id}ã€‚å¤„ç†ä¸­æ­¢ã€‚")
            return [], None

        # 3.2 è·å–æ¼”èŒå‘˜ (ä½¿ç”¨å®Œå…¨å¯ä¿¡çš„ç±»å‹)
        cast_data = self.douban_api.get_acting(
            name=item_name, 
            douban_id_override=douban_id, 
            mtype=douban_type
        )
        douban_cast_raw = cast_data.get("cast", [])

        # 3.3 è·å–è¯¦æƒ…ï¼ˆä¸ºäº†è¯„åˆ†ï¼‰ï¼ŒåŒæ ·ä½¿ç”¨å¯ä¿¡çš„ç±»å‹
        details_data = self.douban_api._get_subject_details(douban_id, douban_type)
        douban_rating = None
        if details_data and not details_data.get("error"):
            rating_str = details_data.get("rating", {}).get("value")
            if rating_str:
                try:
                    douban_rating = float(rating_str)
                    logger.info(f"åœ¨çº¿è·å–åˆ°è±†ç“£è¯„åˆ† for '{item_name}': {douban_rating}")
                except (ValueError, TypeError):
                    pass

        return douban_cast_raw, douban_rating
    # --- é€šè¿‡è±†ç“£IDæŸ¥æ‰¾æ˜ å°„è¡¨ ---
    def _find_person_in_map_by_douban_id(self, douban_id: str, cursor: sqlite3.Cursor) -> Optional[sqlite3.Row]:
        """
        æ ¹æ®è±†ç“£åäººIDåœ¨ person_identity_map è¡¨ä¸­æŸ¥æ‰¾å¯¹åº”çš„è®°å½•ã€‚
        """
        if not douban_id:
            return None
        try:
            cursor.execute(
                "SELECT * FROM person_identity_map WHERE douban_celebrity_id = ?",
                (douban_id,)
            )
            return cursor.fetchone()
        except sqlite3.Error as e:
            logger.error(f"é€šè¿‡è±†ç“£ID '{douban_id}' æŸ¥è¯¢ person_identity_map æ—¶å‡ºé”™: {e}")
            return None
    # --- é€šè¿‡ImbdIDæŸ¥æ‰¾æ˜ å°„è¡¨ ---
    def _find_person_in_map_by_imdb_id(self, imdb_id: str, cursor: sqlite3.Cursor) -> Optional[sqlite3.Row]:
        """
        æ ¹æ® IMDb ID åœ¨ person_identity_map è¡¨ä¸­æŸ¥æ‰¾å¯¹åº”çš„è®°å½•ã€‚
        """
        if not imdb_id:
            return None
        try:
            # æ ¸å¿ƒæ”¹åŠ¨ï¼šå°†æŸ¥è¯¢å­—æ®µä» douban_celebrity_id æ”¹ä¸º imdb_id
            cursor.execute(
                "SELECT * FROM person_identity_map WHERE imdb_id = ?",
                (imdb_id,)
            )
            return cursor.fetchone()
        except sqlite3.Error as e:
            logger.error(f"é€šè¿‡ IMDb ID '{imdb_id}' æŸ¥è¯¢ person_identity_map æ—¶å‡ºé”™: {e}")
            return None
    # --- è¡¥å……æ–°å¢æ¼”å‘˜é¢å¤–æ•°æ® ---
    def _get_actor_metadata_from_cache(self, tmdb_id: int, cursor: sqlite3.Cursor) -> Optional[Dict]:
        """æ ¹æ®TMDb IDä»ActorMetadataç¼“å­˜è¡¨ä¸­è·å–æ¼”å‘˜çš„å…ƒæ•°æ®ã€‚"""
        if not tmdb_id:
            return None
        cursor.execute("SELECT * FROM ActorMetadata WHERE tmdb_id = ?", (tmdb_id,))
        metadata_row = cursor.fetchone()  # fetchone() è¿”å›ä¸€ä¸ª sqlite3.Row å¯¹è±¡æˆ– None
        if metadata_row:
            return dict(metadata_row)  # å°†å…¶è½¬æ¢ä¸ºå­—å…¸ï¼Œæ–¹ä¾¿ä½¿ç”¨
        return None
    # --- æ‰¹é‡æ³¨å…¥åˆ†é›†æ¼”å‘˜è¡¨ ---
    def _batch_update_episodes_cast(self, series_id: str, series_name: str, final_cast_list: List[Dict[str, Any]]):
        """
        ã€V1 - æ‰¹é‡å†™å…¥æ¨¡å—ã€‘
        å°†ä¸€ä¸ªæœ€ç»ˆå¤„ç†å¥½çš„æ¼”å‘˜åˆ—è¡¨ï¼Œé«˜æ•ˆåœ°å†™å…¥æŒ‡å®šå‰§é›†ä¸‹çš„æ‰€æœ‰åˆ†é›†ã€‚
        """
        logger.info(f"ğŸš€ å¼€å§‹ä¸ºå‰§é›† '{series_name}' (ID: {series_id}) æ‰¹é‡æ›´æ–°æ‰€æœ‰åˆ†é›†çš„æ¼”å‘˜è¡¨...")
        
        # 1. è·å–æ‰€æœ‰åˆ†é›†çš„ ID
        # æˆ‘ä»¬åªéœ€è¦ IDï¼Œæ‰€ä»¥å¯ä»¥è¯·æ±‚æ›´å°‘çš„å­—æ®µä»¥æé«˜æ•ˆç‡
        episodes = emby_handler.get_series_children(
            series_id=series_id,
            base_url=self.emby_url,
            api_key=self.emby_api_key,
            user_id=self.emby_user_id,
            series_name_for_log=series_name,
            include_item_types="Episode" # â˜…â˜…â˜… æ˜ç¡®æŒ‡å®šåªè·å–åˆ†é›†
        )
        
        if not episodes:
            logger.info("  -> æœªæ‰¾åˆ°ä»»ä½•åˆ†é›†ï¼Œæ‰¹é‡æ›´æ–°ç»“æŸã€‚")
            return

        total_episodes = len(episodes)
        logger.info(f"  -> å…±æ‰¾åˆ° {total_episodes} ä¸ªåˆ†é›†éœ€è¦æ›´æ–°ã€‚")
        
        # 2. å‡†å¤‡å¥½è¦å†™å…¥çš„æ•°æ® (æ‰€æœ‰åˆ†é›†éƒ½ç”¨åŒä¸€ä»½æ¼”å‘˜è¡¨)
        cast_for_emby_handler = []
        for actor in final_cast_list:
            cast_for_emby_handler.append({
                "name": actor.get("name"),
                "character": actor.get("character"),
                "emby_person_id": actor.get("emby_person_id"),
                "provider_ids": actor.get("provider_ids")
            })

        # 3. éå†å¹¶é€ä¸ªæ›´æ–°åˆ†é›†
        # è¿™é‡Œä»ç„¶éœ€è¦é€ä¸ªæ›´æ–°ï¼Œå› ä¸º Emby API ä¸æ”¯æŒä¸€æ¬¡æ€§æ›´æ–°å¤šä¸ªé¡¹ç›®çš„æ¼”å‘˜è¡¨
        # ä½†æˆ‘ä»¬å·²ç»æŠŠæœ€è€—æ—¶çš„æ•°æ®å¤„ç†æ”¾åœ¨äº†å¾ªç¯å¤–é¢
        for i, episode in enumerate(episodes):
            if self.is_stop_requested():
                logger.warning("åˆ†é›†æ‰¹é‡æ›´æ–°ä»»åŠ¡è¢«ä¸­æ­¢ã€‚")
                break
            
            episode_id = episode.get("Id")
            episode_name = episode.get("Name", f"åˆ†é›† {i+1}")
            logger.debug(f"  ({i+1}/{total_episodes}) æ­£åœ¨æ›´æ–°åˆ†é›† '{episode_name}' (ID: {episode_id})...")
            
            emby_handler.update_emby_item_cast(
                item_id=episode_id,
                new_cast_list_for_handler=cast_for_emby_handler,
                emby_server_url=self.emby_url,
                emby_api_key=self.emby_api_key,
                user_id=self.emby_user_id
            )
            # åŠ å…¥ä¸€ä¸ªå¾®å°çš„å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºå¯†é›†
            time.sleep(0.2)

        logger.info(f"ğŸš€ å‰§é›† '{series_name}' çš„åˆ†é›†æ‰¹é‡æ›´æ–°å®Œæˆã€‚")
    # --- æ ¸å¿ƒå¤„ç†æ€»ç®¡ ---
    def process_single_item(self, emby_item_id: str,
                            force_reprocess_this_item: bool = False,
                            force_fetch_from_tmdb: bool = False):
        """
        ã€V-API-Ready æœ€ç»ˆç‰ˆ - å¸¦è·³è¿‡åŠŸèƒ½ã€‘
        è¿™ä¸ªå‡½æ•°æ˜¯APIæ¨¡å¼çš„å…¥å£ï¼Œå®ƒä¼šå…ˆæ£€æŸ¥æ˜¯å¦éœ€è¦è·³è¿‡å·²å¤„ç†çš„é¡¹ç›®ã€‚
        """
        # 1. é™¤éå¼ºåˆ¶ï¼Œå¦åˆ™è·³è¿‡å·²å¤„ç†çš„
        if not force_reprocess_this_item and emby_item_id in self.processed_items_cache:
            item_name_from_cache = self.processed_items_cache.get(emby_item_id, f"ID:{emby_item_id}")
            logger.info(f"åª’ä½“ '{item_name_from_cache}' è·³è¿‡å·²å¤„ç†è®°å½•ã€‚")
            return True

        # 2. æ£€æŸ¥åœæ­¢ä¿¡å·
        if self.is_stop_requested():
            return False

        # 3. è·å–Embyè¯¦æƒ…ï¼Œè¿™æ˜¯åç»­æ‰€æœ‰æ“ä½œçš„åŸºç¡€
        item_details = emby_handler.get_emby_item_details(emby_item_id, self.emby_url, self.emby_api_key, self.emby_user_id)
        if not item_details:
            logger.error(f"process_single_item: æ— æ³•è·å– Emby é¡¹ç›® {emby_item_id} çš„è¯¦æƒ…ã€‚")
            return False

        # 4. å°†ä»»åŠ¡äº¤ç»™æ ¸å¿ƒå¤„ç†å‡½æ•°
        return self._process_item_core_logic_api_version(
            item_details_from_emby=item_details,
            force_reprocess_this_item=force_reprocess_this_item,
            force_fetch_from_tmdb=force_fetch_from_tmdb
        )

        # --- æ ¸å¿ƒå¤„ç†æµç¨‹ ---
    
    # ---æ ¸å¿ƒå¤„ç†æµç¨‹ ---
    def _process_item_core_logic_api_version(self, item_details_from_emby: Dict[str, Any], force_reprocess_this_item: bool, force_fetch_from_tmdb: bool = False):
        """
        ã€V-Final Clarity - æ¸…æ™°æœ€ç»ˆç‰ˆã€‘
        ç¡®ä¿æ•°æ®æµæ¸…æ™°ã€å•å‘ï¼Œå¹¶ä»æ ¹æºä¸Šè§£å†³æ‰€æœ‰å·²çŸ¥é—®é¢˜ã€‚
        """
        item_id = item_details_from_emby.get("Id")
        item_name_for_log = item_details_from_emby.get("Name", f"æœªçŸ¥é¡¹ç›®(ID:{item_id})")
        tmdb_id = item_details_from_emby.get("ProviderIds", {}).get("Tmdb")
        item_type = item_details_from_emby.get("Type")

        if not tmdb_id:
            logger.error(f"é¡¹ç›® '{item_name_for_log}' ç¼ºå°‘ TMDb IDï¼Œæ— æ³•å¤„ç†ã€‚")
            return False

        try:
            tmdb_details_for_cache = None
            # ======================================================================
            # é˜¶æ®µ 1: Emby ç°çŠ¶æ•°æ®å‡†å¤‡ 
            # ======================================================================
            logger.info(f"  -> å¼€å§‹å¤„ç† '{item_name_for_log}' (TMDb ID: {tmdb_id})")
            
            current_emby_cast_raw = item_details_from_emby.get("People", [])
            enriched_emby_cast = self._enrich_cast_from_db_and_api(current_emby_cast_raw)
            original_emby_actor_count = len(enriched_emby_cast)
            logger.info(f"  -> ä» Emby è·å–åï¼Œå¾—åˆ° {original_emby_actor_count} ä½ç°æœ‰æ¼”å‘˜ç”¨äºåç»­æ‰€æœ‰æ“ä½œã€‚")

            # ======================================================================
            # é˜¶æ®µ 2: æƒå¨æ•°æ®æºé‡‡é›†
            # ======================================================================
            authoritative_cast_source = []

            # â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜… æ ¸å¿ƒæ”¹é€ ï¼šç¡®ä¿æ€»æ˜¯è·å– TMDB è¯¦æƒ… â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…
            # æ— è®ºæ˜¯ä»€ä¹ˆç­–ç•¥ï¼Œæˆ‘ä»¬éƒ½å°è¯•è·å–ä¸€æ¬¡ TMDB è¯¦æƒ…ï¼Œä»¥ä¾¿åç»­ç¼“å­˜
            if self.tmdb_api_key:
                logger.debug("  -> å®æ—¶ç¼“å­˜ï¼šæ­£åœ¨ä¸ºè¡¥å……æ•°æ®ï¼ˆå¯¼æ¼”/å›½å®¶ï¼‰è·å– TMDB è¯¦æƒ…...")
                if item_type == "Movie":
                    tmdb_details_for_cache = tmdb_handler.get_movie_details(tmdb_id, self.tmdb_api_key)
                elif item_type == "Series":
                    tmdb_details_for_cache = tmdb_handler.get_tv_details_tmdb(tmdb_id, self.tmdb_api_key)

            # ç°åœ¨æ‰å¼€å§‹æ ¹æ®ç­–ç•¥å†³å®š authoritative_cast_source
            if force_fetch_from_tmdb and tmdb_details_for_cache:
                logger.info("  -> ç­–ç•¥: å¼ºåˆ¶åˆ·æ–°ï¼Œä½¿ç”¨åˆšä» TMDB è·å–çš„æ•°æ®ä½œä¸ºæƒå¨æ•°æ®æºã€‚")
                # --- ç”µå½±å¤„ç†é€»è¾‘ ---
                if item_type == "Movie":
                    if force_fetch_from_tmdb and self.tmdb_api_key:
                        logger.info("  -> ç”µå½±ç­–ç•¥: å¼ºåˆ¶ä» TMDB API è·å–å…ƒæ•°æ®...")
                        movie_details = tmdb_handler.get_movie_details(tmdb_id, self.tmdb_api_key)
                        if movie_details:
                            credits_data = movie_details.get("credits") or movie_details.get("casts")
                            if credits_data: authoritative_cast_source = credits_data.get("cast", [])
                
                # --- å‰§é›†å¤„ç†é€»è¾‘ ---
                elif item_type == "Series":
                    if force_fetch_from_tmdb and self.tmdb_api_key:
                        logger.info("  -> å‰§é›†ç­–ç•¥: å¼ºåˆ¶ä» TMDB API å¹¶å‘èšåˆ...")
                        aggregated_tmdb_data = tmdb_handler.aggregate_full_series_data_from_tmdb(
                            tv_id=int(tmdb_id), api_key=self.tmdb_api_key, max_workers=5
                        )
                        if aggregated_tmdb_data:
                            all_episodes = list(aggregated_tmdb_data.get("episodes_details", {}).values())
                            authoritative_cast_source = _aggregate_series_cast_from_tmdb_data(aggregated_tmdb_data["series_details"], all_episodes)

            # å¦‚æœå¼ºåˆ¶åˆ·æ–°å¤±è´¥ï¼Œæˆ–è€…æ²¡æœ‰å¼ºåˆ¶åˆ·æ–°ï¼Œåˆ™ä½¿ç”¨æˆ‘ä»¬å·²ç»å¢å¼ºè¿‡çš„ Emby åˆ—è¡¨ä½œä¸ºæƒå¨æ•°æ®æº
            if not authoritative_cast_source:
                logger.info("  -> ä¿åº•ç­–ç•¥: æœªå¼ºåˆ¶åˆ·æ–°æˆ–åˆ·æ–°å¤±è´¥ï¼Œå°†ä½¿ç”¨ Emby æ¼”å‘˜åˆ—è¡¨ä½œä¸ºæƒå¨æ•°æ®æºã€‚")
                authoritative_cast_source = enriched_emby_cast

            logger.info(f"  -> æ•°æ®é‡‡é›†é˜¶æ®µå®Œæˆï¼Œæœ€ç»ˆé€‰å®š {len(authoritative_cast_source)} ä½æƒå¨æ¼”å‘˜ã€‚")

            # ======================================================================
            # é˜¶æ®µ 3: è±†ç“£åŠåç»­å¤„ç†
            # ======================================================================
            douban_cast_raw, _ = self._get_douban_data_with_local_cache(item_details_from_emby)

            with get_central_db_connection(self.db_path) as conn:
                cursor = conn.cursor()
                
                final_processed_cast = self._process_cast_list_from_api(
                    tmdb_cast_people=authoritative_cast_source,
                    emby_cast_people=enriched_emby_cast,
                    douban_cast_list=douban_cast_raw,
                    item_details_from_emby=item_details_from_emby,
                    cursor=cursor,
                    tmdb_api_key=self.tmdb_api_key,
                    stop_event=self.get_stop_event()
                )

                # ======================================================================
                # é˜¶æ®µ 4: æ•°æ®å†™å› (Data Write-back)
                # ======================================================================
                logger.info("  -> å†™å›æ­¥éª¤ 1/2: æ£€æŸ¥å¹¶æ›´æ–°è¢«ç¿»è¯‘çš„æ¼”å‘˜åå­—...")
                # æˆ‘ä»¬éœ€è¦åŸå§‹çš„ Emby æ¼”å‘˜åˆ—è¡¨æ¥è¿›è¡Œåå­—å¯¹æ¯”
                original_names_map = {p.get("Id"): p.get("Name") for p in item_details_from_emby.get("People", []) if p.get("Id")}
                
                for actor in final_processed_cast:
                    if self.is_stop_requested():
                        raise InterruptedError("ä»»åŠ¡åœ¨æ¼”å‘˜åæ›´æ–°é˜¶æ®µè¢«ä¸­æ­¢ã€‚")
                    
                    actor_id = actor.get("emby_person_id")
                    new_name = actor.get("name")
                    original_name = original_names_map.get(actor_id)
                    
                    if actor_id and new_name and original_name and new_name != original_name:
                        logger.info(f"  -> æ£€æµ‹åˆ°æ¼”å‘˜åå˜æ›´ï¼Œæ­£åœ¨æ›´æ–° Person: '{original_name}' -> '{new_name}' (ID: {actor_id})")
                        emby_handler.update_person_details(
                            person_id=actor_id,
                            new_data={"Name": new_name},
                            emby_server_url=self.emby_url,
                            emby_api_key=self.emby_api_key,
                            user_id=self.emby_user_id
                        )
                logger.info("  -> å†™å›æ¼”å‘˜åå­—å‰ç½®æ›´æ–°å®Œæˆã€‚")

                # --- æ›´æ–°åª’ä½“çš„æ¼”å‘˜åˆ—è¡¨ ---
                logger.info("  -> å†™å›æ­¥éª¤ 2/2: å‡†å¤‡å°†æœ€ç»ˆæ¼”å‘˜åˆ—è¡¨æ›´æ–°åˆ°åª’ä½“é¡¹ç›®...")
                cast_for_emby_handler = []
                for actor in final_processed_cast:
                    cast_for_emby_handler.append({
                        "name": actor.get("name"),
                        "character": actor.get("character"),
                        "emby_person_id": actor.get("emby_person_id"),
                        "provider_ids": actor.get("provider_ids")
                    })

                update_success = emby_handler.update_emby_item_cast(
                    item_id=item_id,
                    new_cast_list_for_handler=cast_for_emby_handler,
                    emby_server_url=self.emby_url,
                    emby_api_key=self.emby_api_key,
                    user_id=self.emby_user_id
                )

                if item_type == "Series" and update_success:
                    self._batch_update_episodes_cast(
                        series_id=item_id,
                        series_name=item_name_for_log,
                        final_cast_list=final_processed_cast
                    )

                if not update_success:
                    logger.error(f"æ›´æ–° Emby é¡¹ç›® '{item_name_for_log}' æ¼”å‘˜ä¿¡æ¯å¤±è´¥ï¼Œè®°å½•åˆ°å¾…å¤æ ¸åˆ—è¡¨ã€‚")
                    self.log_db_manager.save_to_failed_log(cursor, item_id, item_name_for_log, "APIæ›´æ–°æ¼”å‘˜ä¿¡æ¯å¤±è´¥", item_type)
                    conn.commit()
                    return False

                # ======================================================================
                # â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜… é˜¶æ®µ 5: è°ƒç”¨â€œé”åŒ â€å®Œæˆæ”¶å°¾ â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…
                # ======================================================================
                auto_lock_enabled = self.config.get(constants.CONFIG_OPTION_AUTO_LOCK_CAST, True)
                fields_to_lock_on_refresh = ["Cast"] if auto_lock_enabled else None
                
                if auto_lock_enabled:
                    logger.info("  -> æ›´æ–°æˆåŠŸï¼Œå°†æ‰§è¡Œåˆ·æ–°å’Œé”å®šæ“ä½œ...")
                else:
                    logger.info("  -> æ›´æ–°æˆåŠŸï¼Œå°†æ‰§è¡Œåˆ·æ–°å’Œè§£é”æ“ä½œ...")
                emby_handler.refresh_emby_item_metadata(
                    item_emby_id=item_id,
                    emby_server_url=self.emby_url,
                    emby_api_key=self.emby_api_key,
                    user_id_for_ops=self.emby_user_id,
                    lock_fields=fields_to_lock_on_refresh, # â˜… æŠŠè¦é”å®šçš„å­—æ®µä¼ è¿›å»
                    replace_all_metadata_param=False, # è½»é‡çº§åˆ·æ–°
                    item_name_for_log=item_name_for_log
                )

                # ======================================================================
                # é˜¶æ®µ 4: å®æ—¶å…ƒæ•°æ®ç¼“å­˜ (ç°åœ¨æ€»æ˜¯èƒ½æ‰§è¡Œäº†)
                # ======================================================================
                logger.info(f"  -> å®æ—¶ç¼“å­˜ï¼šå‡†å¤‡å°† '{item_name_for_log}' çš„å…ƒæ•°æ®å†™å…¥æœ¬åœ°æ•°æ®åº“...")
                _save_metadata_to_cache(
                    cursor=cursor,
                    tmdb_id=tmdb_id,
                    item_type=item_type,
                    item_details_from_emby=item_details_from_emby,
                    final_processed_cast=final_processed_cast,
                    tmdb_details_for_extra=tmdb_details_for_cache # â˜… æŠŠæˆ‘ä»¬è·å–åˆ°çš„è¡¥å……æ•°æ®ä¼ è¿›å»
                )

                # ======================================================================
                # é˜¶æ®µ 6: åç»­å¤„ç† (Post-processing)
                # ======================================================================
                genres = item_details_from_emby.get("Genres", [])
                is_animation = "Animation" in genres or "åŠ¨ç”»" in genres or "Documentary" in genres or "çºªå½•" in genres
                processing_score = actor_utils.evaluate_cast_processing_quality(
                    final_cast=final_processed_cast,
                    original_cast_count=original_emby_actor_count,
                    expected_final_count=len(final_processed_cast),
                    is_animation=is_animation
                )

                min_score_for_review = float(self.config.get("min_score_for_review", constants.DEFAULT_MIN_SCORE_FOR_REVIEW))
                if processing_score < min_score_for_review:
                    reason = f"å¤„ç†è¯„åˆ† ({processing_score:.2f}) ä½äºé˜ˆå€¼ ({min_score_for_review})ã€‚"
                    self.log_db_manager.save_to_failed_log(cursor, item_id, item_name_for_log, reason, item_type, score=processing_score)
                else:
                    self.log_db_manager.save_to_processed_log(cursor, item_id, item_name_for_log, score=processing_score)
                    self.log_db_manager.remove_from_failed_log(cursor, item_id)
                    self.processed_items_cache[item_id] = item_name_for_log
                    logger.debug(f"å·²å°† '{item_name_for_log}' (ID: {item_id}) æ·»åŠ åˆ°å·²å¤„ç†ï¼Œä¸‹æ¬¡å°†è·³è¿‡ã€‚")

                conn.commit()

        except (ValueError, InterruptedError) as e:
            logger.warning(f"å¤„ç† '{item_name_for_log}' çš„è¿‡ç¨‹ä¸­æ–­: {e}")
            return False
        except Exception as outer_e:
            logger.error(f"APIæ¨¡å¼æ ¸å¿ƒå¤„ç†æµç¨‹ä¸­å‘ç”ŸæœªçŸ¥ä¸¥é‡é”™è¯¯ for '{item_name_for_log}': {outer_e}", exc_info=True)
            try:
                with get_central_db_connection(self.db_path) as conn_fail:
                    self.log_db_manager.save_to_failed_log(conn_fail.cursor(), item_id, item_name_for_log, f"æ ¸å¿ƒå¤„ç†å¼‚å¸¸: {str(outer_e)}", item_type)
            except Exception as log_e:
                logger.error(f"å†™å…¥å¤±è´¥æ—¥å¿—æ—¶å†æ¬¡å‘ç”Ÿé”™è¯¯: {log_e}")
            return False

        logger.info(f"âœ¨âœ¨âœ¨ API æ¨¡å¼å¤„ç†å®Œæˆ '{item_name_for_log}' âœ¨âœ¨âœ¨")
        return True

    # --- æ ¸å¿ƒå¤„ç†å™¨ ---
    def _process_cast_list_from_api(self, tmdb_cast_people: List[Dict[str, Any]],
                                    emby_cast_people: List[Dict[str, Any]],
                                    douban_cast_list: List[Dict[str, Any]],
                                    item_details_from_emby: Dict[str, Any],
                                    cursor: sqlite3.Cursor,
                                    tmdb_api_key: Optional[str],
                                    stop_event: Optional[threading.Event]) -> List[Dict[str, Any]]:
        """
        åœ¨å‡½æ•°å¼€å¤´å¢åŠ ä¸€ä¸ªâ€œæ•°æ®é€‚é…å±‚â€ï¼Œå°†APIæ•°æ®è½¬æ¢ä¸ºä½ ç°æœ‰é€»è¾‘æœŸæœ›çš„æ ¼å¼ï¼Œ
        ç„¶ååŸå°ä¸åŠ¨åœ°æ‰§è¡Œä½ æ‰€æœ‰ç»è¿‡æ‰“ç£¨çš„æ ¸å¿ƒä»£ç ã€‚
        """
        logger.debug("APIæ¨¡å¼ï¼šè¿›å…¥æ•°æ®é€‚é…å±‚...")
        emby_tmdb_to_person_id_map = {
            person.get("ProviderIds", {}).get("Tmdb"): person.get("Id")
            for person in emby_cast_people if person.get("ProviderIds", {}).get("Tmdb")
        }

        local_cast_list = []
        for person_data in tmdb_cast_people: # tmdb_cast_people ç°åœ¨æ˜¯ authoritative_cast_source
            
            # æ™ºèƒ½åœ°ä»æ•°æ®æºä¸­æå– TMDB ID
            tmdb_id = None
            # ä¼˜å…ˆæ£€æŸ¥ TMDB/ç¥åŒ»ç¼“å­˜ æ ‡å‡†æ ¼å¼
            if "id" in person_data:
                tmdb_id = str(person_data.get("id"))
            # å…¶æ¬¡æ£€æŸ¥ Emby People åˆ—è¡¨æ ¼å¼
            elif "ProviderIds" in person_data and person_data.get("ProviderIds", {}).get("Tmdb"):
                tmdb_id = str(person_data["ProviderIds"]["Tmdb"])
            
            if not tmdb_id or tmdb_id == 'None':
                continue

            new_actor_entry = person_data.copy()
            
            # æ³¨å…¥ emby_person_id
            new_actor_entry["emby_person_id"] = emby_tmdb_to_person_id_map.get(tmdb_id)
            
            # ç»Ÿä¸€æ•°æ®ç»“æ„ï¼Œç¡®ä¿ä¸‹æ¸¸ä»£ç èƒ½æ­£å¸¸å·¥ä½œ
            if "id" not in new_actor_entry:
                new_actor_entry["id"] = tmdb_id
            if "name" not in new_actor_entry:
                new_actor_entry["name"] = new_actor_entry.get("Name")
            if "character" not in new_actor_entry:
                new_actor_entry["character"] = new_actor_entry.get("Role")

            local_cast_list.append(new_actor_entry)
        # â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜… å…¨æ–°çš„ã€æ›´æ™ºèƒ½çš„æ•°æ®é€‚é…å±‚ END â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…

        logger.debug(f"æ•°æ®é€‚é…å®Œæˆï¼Œç”Ÿæˆäº† {len(local_cast_list)} æ¡åŸºå‡†æ¼”å‘˜æ•°æ®ã€‚")
        # ======================================================================
        # æ­¥éª¤ 2: â˜…â˜…â˜… åŸå°ä¸åŠ¨åœ°æ‰§è¡Œä½ æ‰€æœ‰çš„â€œåŸå‚é€»è¾‘â€ â˜…â˜…â˜…
        # (ä¸‹é¢çš„ä»£ç ï¼Œæ˜¯æˆ‘æ ¹æ®ä½ ä¸Šæ¬¡å‘çš„å‡½æ•°ï¼Œæ•´ç†å‡ºçš„æœ€æ¥è¿‘ä½ åŸç‰ˆçš„é€»è¾‘)
        # ======================================================================

        douban_candidates = actor_utils.format_douban_cast(douban_cast_list)

        # --- ä½ çš„â€œä¸€å¯¹ä¸€åŒ¹é…â€é€»è¾‘ ---
        unmatched_local_actors = list(local_cast_list)  # â˜…â˜…â˜… ä½¿ç”¨æˆ‘ä»¬é€‚é…å¥½çš„æ•°æ®æº â˜…â˜…â˜…
        merged_actors = []
        unmatched_douban_actors = []
        # 3. éå†è±†ç“£æ¼”å‘˜ï¼Œå°è¯•åœ¨â€œæœªåŒ¹é…â€çš„æœ¬åœ°æ¼”å‘˜ä¸­å¯»æ‰¾é…å¯¹
        for d_actor in douban_candidates:
            douban_name_zh = d_actor.get("Name", "").lower().strip()
            douban_name_en = d_actor.get("OriginalName", "").lower().strip()

            match_found_for_this_douban_actor = False

            for i, l_actor in enumerate(unmatched_local_actors):
                local_name = str(l_actor.get("name") or "").lower().strip()
                local_original_name = str(l_actor.get("original_name") or "").lower().strip()
                is_match, match_reason = False, ""
                if douban_name_zh and (douban_name_zh == local_name or douban_name_zh == local_original_name):
                    is_match, match_reason = True, "ç²¾ç¡®åŒ¹é… (è±†ç“£ä¸­æ–‡å)"
                elif douban_name_en and (douban_name_en == local_name or douban_name_en == local_original_name):
                    is_match, match_reason = True, "ç²¾ç¡®åŒ¹é… (è±†ç“£å¤–æ–‡å)"
                if is_match:
                    logger.debug(f"  åŒ¹é…æˆåŠŸ (å¯¹å·å…¥åº§): è±†ç“£æ¼”å‘˜ '{d_actor.get('Name')}' -> æœ¬åœ°æ¼”å‘˜ '{l_actor.get('name')}' (ID: {l_actor.get('id')})")

                    l_actor["name"] = d_actor.get("Name")
                    cleaned_douban_character = utils.clean_character_name_static(d_actor.get("Role"))
                    l_actor["character"] = actor_utils.select_best_role(l_actor.get("character"), cleaned_douban_character)
                    if d_actor.get("DoubanCelebrityId"):
                        l_actor["douban_id"] = d_actor.get("DoubanCelebrityId")

                    merged_actors.append(unmatched_local_actors.pop(i))
                    match_found_for_this_douban_actor = True
                    break

            if not match_found_for_this_douban_actor:
                unmatched_douban_actors.append(d_actor)

        # è¿™é‡Œå…ˆæŠŠæ—§æ¼”å‘˜åˆå¹¶æˆåˆ—è¡¨ï¼Œä¾›åç»­æ–°å¢å’Œå¤„ç†ä½¿ç”¨
        current_cast_list = merged_actors + unmatched_local_actors

        # â˜…â˜…â˜… ã€æ ¸å¿ƒä¿®å¤ï¼šæŠŠæ–°å¢æ¼”å‘˜ç›´æ¥åŠ å…¥current_cast_listï¼Œç»Ÿä¸€å¤„ç†ã€‘ â˜…â˜…â˜…
        # å…ˆæ„é€  final_cast_mapï¼ŒåŒ…å«æ—§æ¼”å‘˜
        final_cast_map = {str(actor['id']): actor for actor in current_cast_list if actor.get('id') and str(actor.get('id')) != 'None'}
        # æ–°å¢é˜¶æ®µå¼€å§‹
        limit = self.config.get(constants.CONFIG_OPTION_MAX_ACTORS_TO_PROCESS, 30)
        try:
            limit = int(limit)
            if limit <= 0:
                limit = 30
        except (ValueError, TypeError):
            limit = 30

        current_actor_count = len(final_cast_map)
        if current_actor_count >= limit:
            logger.info(f"å½“å‰æ¼”å‘˜æ•° ({current_actor_count}) å·²è¾¾ä¸Šé™ ({limit})ï¼Œè·³è¿‡æ‰€æœ‰æ–°å¢æ¼”å‘˜çš„æµç¨‹ã€‚")
        else:
            logger.info(f"å½“å‰æ¼”å‘˜æ•° ({current_actor_count}) ä½äºä¸Šé™ ({limit})ï¼Œè¿›å…¥è¡¥å……æ¨¡å¼ï¼ˆå¤„ç†æ¥è‡ªè±†ç“£çš„æ–°å¢æ¼”å‘˜ï¼‰ã€‚")

            logger.debug(f"--- åŒ¹é…é˜¶æ®µ 2: ç”¨è±†ç“£IDæŸ¥ person_identity_map ({len(unmatched_douban_actors)} ä½æ¼”å‘˜) ---")
            still_unmatched = []
            for d_actor in unmatched_douban_actors:
                if self.is_stop_requested():
                    raise InterruptedError("ä»»åŠ¡ä¸­æ­¢")
                d_douban_id = d_actor.get("DoubanCelebrityId")
                match_found = False
                if d_douban_id:
                    entry = self._find_person_in_map_by_douban_id(d_douban_id, cursor)
                    if entry and entry["tmdb_person_id"]:
                        tmdb_id_from_map = str(entry["tmdb_person_id"])
                        if tmdb_id_from_map not in final_cast_map:
                            logger.debug(f"  åŒ¹é…æˆåŠŸ (é€šè¿‡ è±†ç“£IDæ˜ å°„): è±†ç“£æ¼”å‘˜ '{d_actor.get('Name')}' -> åŠ å…¥æœ€ç»ˆæ¼”å‘˜è¡¨")
                            cached_metadata = self._get_actor_metadata_from_cache(tmdb_id_from_map, cursor) or {}
                            new_actor_entry = {
                                "id": tmdb_id_from_map,
                                "name": d_actor.get("Name"),
                                "original_name": cached_metadata.get("original_name") or d_actor.get("OriginalName"),
                                "character": d_actor.get("Role"),
                                "adult": cached_metadata.get("adult", False),
                                "gender": cached_metadata.get("gender", 0),
                                "known_for_department": "Acting",
                                "popularity": cached_metadata.get("popularity", 0.0),
                                "profile_path": cached_metadata.get("profile_path"),
                                "cast_id": None,
                                "credit_id": None,
                                "order": 999,
                                "imdb_id": entry["imdb_id"] if "imdb_id" in entry else None,
                                "douban_id": d_douban_id,
                                "_is_newly_added": True
                            }
                            final_cast_map[tmdb_id_from_map] = new_actor_entry
                        match_found = True
                if not match_found:
                    still_unmatched.append(d_actor)
            unmatched_douban_actors = still_unmatched

            # --- æ­¥éª¤ 3 & 4: IMDb ID åæŸ¥ æ–°å¢æ“ä½œ ---
            logger.debug(f"--- åŒ¹é…é˜¶æ®µ 3 & 4: ç”¨IMDb IDè¿›è¡Œæœ€ç»ˆåŒ¹é…å’Œæ–°å¢ ({len(unmatched_douban_actors)} ä½æ¼”å‘˜) ---")
            still_unmatched_final = []
            for i, d_actor in enumerate(unmatched_douban_actors):
                if self.is_stop_requested():
                    raise InterruptedError("ä»»åŠ¡ä¸­æ­¢")

                if len(final_cast_map) >= limit:
                    logger.info(f"æ¼”å‘˜æ•°å·²è¾¾ä¸Šé™ ({limit})ï¼Œè·³è¿‡å‰©ä½™ {len(unmatched_douban_actors) - i} ä½æ¼”å‘˜çš„APIæŸ¥è¯¢ã€‚")
                    still_unmatched_final.extend(unmatched_douban_actors[i:])
                    break
                d_douban_id = d_actor.get("DoubanCelebrityId")
                match_found = False
                if d_douban_id and self.douban_api and self.tmdb_api_key:
                    if self.is_stop_requested():
                        logger.info("ä»»åŠ¡åœ¨å¤„ç†è±†ç“£æ¼”å‘˜æ—¶è¢«ä¸­æ­¢ (è±†ç“£APIè°ƒç”¨å‰)ã€‚")
                        raise InterruptedError("ä»»åŠ¡ä¸­æ­¢")
                    details = self.douban_api.celebrity_details(d_douban_id)
                    time.sleep(0.3)

                    d_imdb_id = None
                    if details and not details.get("error"):
                        try:
                            info_list = details.get("extra", {}).get("info", [])
                            if isinstance(info_list, list):
                                for item in info_list:
                                    if isinstance(item, list) and len(item) == 2 and item[0] == 'IMDbç¼–å·':
                                        d_imdb_id = item[1]
                                        break
                        except Exception as e_parse:
                            logger.warning(f"    -> è§£æ IMDb ID æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e_parse}")

                    if d_imdb_id:
                        logger.debug(f"    -> ä¸º '{d_actor.get('Name')}' è·å–åˆ° IMDb ID: {d_imdb_id}ï¼Œå¼€å§‹åŒ¹é…...")

                        entry_from_map = self._find_person_in_map_by_imdb_id(d_imdb_id, cursor)

                        if entry_from_map and entry_from_map["tmdb_person_id"]:
                            tmdb_id_from_map = str(entry_from_map["tmdb_person_id"])

                            if tmdb_id_from_map not in final_cast_map:
                                logger.debug(f"  åŒ¹é…æˆåŠŸ (é€šè¿‡ IMDbæ˜ å°„): è±†ç“£æ¼”å‘˜ '{d_actor.get('Name')}' -> åŠ å…¥æœ€ç»ˆæ¼”å‘˜è¡¨")
                                cached_metadata = self._get_actor_metadata_from_cache(tmdb_id_from_map, cursor) or {}
                                new_actor_entry = {
                                    "id": tmdb_id_from_map,
                                    "name": d_actor.get("Name"),
                                    "original_name": cached_metadata.get("original_name") or d_actor.get("OriginalName"),
                                    "character": d_actor.get("Role"),
                                    "adult": cached_metadata.get("adult", False),
                                    "gender": cached_metadata.get("gender", 0),
                                    "known_for_department": "Acting",
                                    "popularity": cached_metadata.get("popularity", 0.0),
                                    "profile_path": cached_metadata.get("profile_path"),
                                    "cast_id": None,
                                    "credit_id": None,
                                    "order": 999,
                                    "imdb_id": d_imdb_id,
                                    "douban_id": d_douban_id,
                                    "_is_newly_added": True
                                }
                                final_cast_map[tmdb_id_from_map] = new_actor_entry

                            logger.debug(f"    -> [å®æ—¶åå“º] å°†æ–°å‘ç°çš„æ˜ å°„å…³ç³» (Douban ID: {d_douban_id}) ä¿å­˜å›æ•°æ®åº“...")
                            self.actor_db_manager.upsert_person(
                                cursor,
                                {
                                    "tmdb_id": tmdb_id_from_map,
                                    "imdb_id": d_imdb_id,
                                    "douban_id": d_douban_id,
                                     "name": d_actor.get("Name") or (entry_from_map["primary_name"] if "primary_name" in entry_from_map else None)
                                }
                            )
                            match_found = True

                        if not match_found:
                            logger.debug(f"    -> æ•°æ®åº“æœªæ‰¾åˆ° {d_imdb_id} çš„æ˜ å°„ï¼Œå¼€å§‹é€šè¿‡ TMDb API åæŸ¥...")
                            if self.is_stop_requested():
                                logger.info("ä»»åŠ¡åœ¨å¤„ç†è±†ç“£æ¼”å‘˜æ—¶è¢«ä¸­æ­¢ (TMDb APIè°ƒç”¨å‰)ã€‚")
                                raise InterruptedError("ä»»åŠ¡ä¸­æ­¢")

                            person_from_tmdb = tmdb_handler.find_person_by_external_id(d_imdb_id, self.tmdb_api_key, "imdb_id")
                            if person_from_tmdb and person_from_tmdb.get("id"):
                                tmdb_id_from_find = str(person_from_tmdb.get("id"))

                                if tmdb_id_from_find not in final_cast_map:
                                    logger.debug(f"  åŒ¹é…æˆåŠŸ (é€šè¿‡ TMDbåæŸ¥): è±†ç“£æ¼”å‘˜ '{d_actor.get('Name')}' -> åŠ å…¥æœ€ç»ˆæ¼”å‘˜è¡¨")
                                    cached_metadata = self._get_actor_metadata_from_cache(tmdb_id_from_find, cursor) or {}
                                    new_actor_entry = {
                                        "id": tmdb_id_from_find,
                                        "name": d_actor.get("Name"),
                                        "original_name": cached_metadata.get("original_name") or d_actor.get("OriginalName"),
                                        "character": d_actor.get("Role"),
                                        "adult": cached_metadata.get("adult", False),
                                        "gender": cached_metadata.get("gender", 0),
                                        "known_for_department": "Acting",
                                        "popularity": cached_metadata.get("popularity", 0.0),
                                        "profile_path": cached_metadata.get("profile_path"),
                                        "cast_id": None,
                                        "credit_id": None,
                                        "order": 999,
                                        "imdb_id": d_imdb_id,
                                        "douban_id": d_douban_id,
                                        "_is_newly_added": True
                                    }
                                    final_cast_map[tmdb_id_from_find] = new_actor_entry
                                    self.actor_db_manager.upsert_person(
                                        cursor,
                                        {
                                            "tmdb_id": tmdb_id_from_find,
                                            "imdb_id": d_imdb_id,
                                            "douban_id": d_douban_id,
                                            "name": d_actor.get("Name")
                                        }
                                    )
                                match_found = True
                if not match_found:
                    still_unmatched_final.append(d_actor)
            if still_unmatched_final:
                discarded_names = [d.get('Name') for d in still_unmatched_final]
                logger.info(f"--- æœ€ç»ˆä¸¢å¼ƒ {len(still_unmatched_final)} ä½æ— åŒ¹é…çš„è±†ç“£æ¼”å‘˜ ---")
            unmatched_douban_actors = still_unmatched_final

        # å°†æœ€ç»ˆæ¼”å‘˜åˆ—è¡¨å–è‡ª final_cast_mapï¼ŒåŒ…å«æ‰€æœ‰æ—§ï¼‹æ–°æ¼”å‘˜
        current_cast_list = list(final_cast_map.values())

        # â˜…â˜…â˜… åœ¨æˆªæ–­å‰è¿›è¡Œä¸€æ¬¡å…¨é‡åå“º â˜…â˜…â˜…
        logger.debug(f"æˆªæ–­å‰ï¼šå°† {len(current_cast_list)} ä½æ¼”å‘˜çš„å®Œæ•´æ˜ å°„å…³ç³»åå“ºåˆ°æ•°æ®åº“...")
        for actor_data in current_cast_list:
            self.actor_db_manager.upsert_person(
                cursor,
                {
                    "tmdb_id": actor_data.get("id"),
                    "name": actor_data.get("name"),
                    "imdb_id": actor_data.get("imdb_id"),
                    "douban_id": actor_data.get("douban_id"),
                },
            )
        logger.trace("æ‰€æœ‰æ¼”å‘˜çš„IDæ˜ å°„å…³ç³»å·²ä¿å­˜ã€‚")

        # æ­¥éª¤ æ¼”å‘˜åˆ—è¡¨æˆªæ–­ (å…ˆæˆªæ–­ï¼)
        max_actors = self.config.get(constants.CONFIG_OPTION_MAX_ACTORS_TO_PROCESS, 30)
        try:
            limit = int(max_actors)
            if limit <= 0:
                limit = 30
        except (ValueError, TypeError):
            limit = 30

        original_count = len(current_cast_list)
        if original_count > limit:
            logger.info(f"æ¼”å‘˜åˆ—è¡¨æ€»æ•° ({original_count}) è¶…è¿‡ä¸Šé™ ({limit})ï¼Œå°†åœ¨ç¿»è¯‘å‰è¿›è¡Œæˆªæ–­ã€‚")
            # æŒ‰ order æ’åº
            current_cast_list.sort(key=lambda x: x.get('order') if x.get('order') is not None and x.get('order') >= 0 else 999)
            cast_to_process = current_cast_list[:limit]
        else:
            cast_to_process = current_cast_list

        logger.info(f"å°†å¯¹ {len(cast_to_process)} ä½æ¼”å‘˜è¿›è¡Œæœ€ç»ˆçš„ç¿»è¯‘å’Œæ ¼å¼åŒ–å¤„ç†...")

        # ======================================================================
        # æ­¥éª¤ B: ç¿»è¯‘å‡†å¤‡ä¸æ‰§è¡Œ (åæ”¶é›†ï¼Œå¹¶æ£€æŸ¥ç¼“å­˜ï¼)
        # ======================================================================
        ai_translation_succeeded = False
        translation_cache = {}  # â˜…â˜…â˜… æ ¸å¿ƒä¿®æ­£1ï¼šå°†ç¼“å­˜åˆå§‹åŒ–åœ¨æœ€å¤–é¢
        texts_to_collect = set()
        texts_to_send_to_api = set()

        if self.ai_translator and self.config.get(constants.CONFIG_OPTION_AI_TRANSLATION_ENABLED, False):
            logger.info("AIç¿»è¯‘å·²å¯ç”¨ï¼Œä¼˜å…ˆå°è¯•æ‰¹é‡ç¿»è¯‘æ¨¡å¼ã€‚")

            try:
                translation_mode = self.config.get(constants.CONFIG_OPTION_AI_TRANSLATION_MODE, "fast")

                for actor in cast_to_process:
                    name = actor.get('name')
                    if name and not utils.contains_chinese(name):
                        texts_to_collect.add(name)

                    character = actor.get('character')
                    if character:
                        cleaned_character = utils.clean_character_name_static(character)
                        if cleaned_character and not utils.contains_chinese(cleaned_character):
                            texts_to_collect.add(cleaned_character)

                if translation_mode == 'fast':
                    logger.debug("[ç¿»è¯‘æ¨¡å¼] æ­£åœ¨æ£€æŸ¥å…¨å±€ç¿»è¯‘ç¼“å­˜...")
                    for text in texts_to_collect:
                        cached_entry = self.actor_db_manager.get_translation_from_db(cursor=cursor, text=text)
                        if cached_entry:
                            translation_cache[text] = cached_entry.get("translated_text")
                        else:
                            texts_to_send_to_api.add(text)
                else:
                    logger.debug("[é¡¾é—®æ¨¡å¼] è·³è¿‡ç¼“å­˜æ£€æŸ¥ï¼Œç›´æ¥ç¿»è¯‘æ‰€æœ‰è¯æ¡ã€‚")
                    texts_to_send_to_api = texts_to_collect
                if texts_to_send_to_api:
                    item_title = item_details_from_emby.get("Name")
                    item_year = item_details_from_emby.get("ProductionYear")

                    logger.info(f"å°† {len(texts_to_send_to_api)} ä¸ªè¯æ¡æäº¤ç»™AI (æ¨¡å¼: {translation_mode})ã€‚")

                    translation_map_from_api = self.ai_translator.batch_translate(
                        texts=list(texts_to_send_to_api),
                        mode=translation_mode,
                        title=item_title,
                        year=item_year
                    )

                    if translation_map_from_api:
                        translation_cache.update(translation_map_from_api)
                        if translation_mode == 'fast':
                            for original, translated in translation_map_from_api.items():
                                self.actor_db_manager.save_translation_to_db(
                                    cursor=cursor,
                                    original_text=original,
                                    translated_text=translated,
                                    engine_used=self.ai_translator.provider
                                )

                ai_translation_succeeded = True
            except Exception as e:
                logger.error(f"è°ƒç”¨AIæ‰¹é‡ç¿»è¯‘æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
                ai_translation_succeeded = False
        else:
            logger.info("AIç¿»è¯‘æœªå¯ç”¨ï¼Œå°†ä¿ç•™æ¼”å‘˜å’Œè§’è‰²ååŸæ–‡ã€‚")

        # --- â˜…â˜…â˜… æ ¸å¿ƒä¿®æ­£2ï¼šæ— è®ºAIæ˜¯å¦æˆåŠŸï¼Œéƒ½æ‰§è¡Œæ¸…ç†ä¸å›å¡«ï¼Œé™çº§é€»è¾‘åªåœ¨AIå¤±è´¥æ—¶è§¦å‘ â˜…â˜…â˜…

        if ai_translation_succeeded:
            logger.info("------------ AIç¿»è¯‘æµç¨‹æˆåŠŸï¼Œå¼€å§‹åº”ç”¨ç»“æœ ------------")

            if not texts_to_collect:
                logger.info("  æ‰€æœ‰æ¼”å‘˜åå’Œè§’è‰²åå‡å·²æ˜¯ä¸­æ–‡ï¼Œæ— éœ€ç¿»è¯‘ã€‚")
            elif not texts_to_send_to_api:
                logger.info(f"  æ‰€æœ‰ {len(texts_to_collect)} ä¸ªå¾…ç¿»è¯‘è¯æ¡å‡ä»æ•°æ®åº“ç¼“å­˜ä¸­è·å–ï¼Œæ— éœ€è°ƒç”¨AIã€‚")
            else:
                logger.info(f"  AIç¿»è¯‘å®Œæˆï¼Œå…±å¤„ç† {len(translation_cache)} ä¸ªè¯æ¡ã€‚")

            # æ— æ¡ä»¶æ‰§è¡Œå›å¡«ï¼Œå› ä¸ºtranslation_cacheåŒ…å«æ‰€æœ‰éœ€æ•°æ®ï¼ˆæ¥è‡ªç¼“å­˜æˆ–APIï¼‰ã€‚
            for actor in cast_to_process:
                # 1. å¤„ç†æ¼”å‘˜å
                original_name = actor.get('name')
                translated_name = translation_cache.get(original_name, original_name)
                if original_name != translated_name:
                    logger.debug(f"  æ¼”å‘˜åç¿»è¯‘: '{original_name}' -> '{translated_name}'")
                actor['name'] = translated_name

                # 2. å¤„ç†è§’è‰²å
                original_character = actor.get('character')
                if original_character:
                    cleaned_character = utils.clean_character_name_static(original_character)
                    translated_character = translation_cache.get(cleaned_character, cleaned_character)
                    if translated_character != original_character:
                        actor_name_for_log = actor.get('name', 'æœªçŸ¥æ¼”å‘˜')
                        logger.debug(f"  è§’è‰²åç¿»è¯‘: '{original_character}' -> '{translated_character}' (æ¼”å‘˜: {actor_name_for_log})")
                    actor['character'] = translated_character
                else:
                    # ä¿è¯å­—æ®µå§‹ç»ˆæœ‰å­—ç¬¦ä¸²ï¼Œé¿å…æ¼ç½‘
                    actor['character'] = ''

            logger.info("----------------------------------------------------")
        else:
            # AIå¤±è´¥æ—¶ä¿ç•™åŸæ–‡ï¼Œä¸åšç¿»è¯‘æ”¹å†™
            if self.config.get(constants.CONFIG_OPTION_AI_TRANSLATION_ENABLED, False):
                logger.warning("AIæ‰¹é‡ç¿»è¯‘å¤±è´¥ï¼Œå°†ä¿ç•™æ¼”å‘˜å’Œè§’è‰²ååŸæ–‡ã€‚")

        # 3.1 ã€åŠ©ç†ä¸Šåœºã€‘åœ¨æ ¼å¼åŒ–å‰ï¼Œå¤‡ä»½æ‰€æœ‰å·¥ç‰Œ (emby_person_id)
        logger.debug("è°ƒç”¨ actor_utils.format_and_complete_cast_list è¿›è¡Œæœ€ç»ˆæ ¼å¼åŒ–...")
        is_animation = "Animation" in item_details_from_emby.get("Genres", [])
        final_cast_perfect = actor_utils.format_and_complete_cast_list(
            cast_to_process, is_animation, self.config, mode='auto'
        )

        # 3.2 ã€åŠ©ç†æ”¶å°¾ã€‘ç›´æ¥å‡†å¤‡ provider_ids
        # emby_person_id å·²ç»ç”±ä¸Šä¸€æ­¥å‡½æ•°å®Œæ•´åœ°ä¿ç•™ä¸‹æ¥äº†
        logger.debug("æ ¼å¼åŒ–å®Œæˆï¼Œå‡†å¤‡æœ€ç»ˆçš„ provider_ids...")
        for actor in final_cast_perfect:
            # é¡ºä¾¿æŠŠ provider_ids å‡†å¤‡å¥½ï¼Œä¸‹æ¸¸å‡½æ•°ä¼šç”¨åˆ°
            actor["provider_ids"] = {
                "Tmdb": str(actor.get("id")), # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²
                "Imdb": actor.get("imdb_id"),
                "Douban": actor.get("douban_id")
            }
            # (å¯é€‰) å¢åŠ ä¸€æ¡è¯Šæ–­æ—¥å¿—ï¼Œç¡®è®¤ emby_person_id çœŸçš„è¿˜åœ¨
            if actor.get("emby_person_id"):
                logger.trace(f"  æ¼”å‘˜ '{actor.get('name')}' ä¿ç•™äº† Emby Person ID: {actor.get('emby_person_id')}")

        return final_cast_perfect

    def process_full_library(self, update_status_callback: Optional[callable] = None, force_reprocess_all: bool = False, force_fetch_from_tmdb: bool = False):
        """
        ã€V3 - æœ€ç»ˆå®Œæ•´ç‰ˆã€‘
        è¿™æ˜¯æ‰€æœ‰å…¨é‡å¤„ç†çš„å”¯ä¸€å…¥å£ï¼Œå®ƒè‡ªå·±å¤„ç†æ‰€æœ‰ä¸â€œå¼ºåˆ¶â€ç›¸å…³çš„é€»è¾‘ã€‚
        """
        self.clear_stop_signal()
        
        logger.info(f"è¿›å…¥æ ¸å¿ƒæ‰§è¡Œå±‚: process_full_library, æ¥æ”¶åˆ°çš„ force_reprocess_all = {force_reprocess_all}, force_fetch_from_tmdb = {force_fetch_from_tmdb}")

        if force_reprocess_all:
            logger.info("æ£€æµ‹åˆ°â€œå¼ºåˆ¶é‡å¤„ç†â€é€‰é¡¹ï¼Œæ­£åœ¨æ¸…ç©ºå·²å¤„ç†æ—¥å¿—...")
            try:
                self.clear_processed_log()
            except Exception as e:
                logger.error(f"åœ¨ process_full_library ä¸­æ¸…ç©ºæ—¥å¿—å¤±è´¥: {e}", exc_info=True)
                if update_status_callback: update_status_callback(-1, "æ¸…ç©ºæ—¥å¿—å¤±è´¥")
                return

        # --- â˜…â˜…â˜… è¡¥å…¨äº†è¿™éƒ¨åˆ†ä»£ç  â˜…â˜…â˜… ---
        libs_to_process_ids = self.config.get("libraries_to_process", [])
        if not libs_to_process_ids:
            logger.warning("æœªåœ¨é…ç½®ä¸­æŒ‡å®šè¦å¤„ç†çš„åª’ä½“åº“ã€‚")
            return

        logger.info("æ­£åœ¨å°è¯•ä»Embyè·å–åª’ä½“é¡¹ç›®...")
        all_emby_libraries = emby_handler.get_emby_libraries(self.emby_url, self.emby_api_key, self.emby_user_id) or []
        library_name_map = {lib.get('Id'): lib.get('Name', 'æœªçŸ¥åº“å') for lib in all_emby_libraries}
        
        movies = emby_handler.get_emby_library_items(self.emby_url, self.emby_api_key, "Movie", self.emby_user_id, libs_to_process_ids, library_name_map=library_name_map) or []
        series = emby_handler.get_emby_library_items(self.emby_url, self.emby_api_key, "Series", self.emby_user_id, libs_to_process_ids, library_name_map=library_name_map) or []
        
        if movies:
            source_movie_lib_names = sorted(list({library_name_map.get(item.get('_SourceLibraryId')) for item in movies if item.get('_SourceLibraryId')}))
            logger.info(f"ä»åª’ä½“åº“ã€{', '.join(source_movie_lib_names)}ã€‘è·å–åˆ° {len(movies)} ä¸ªç”µå½±é¡¹ç›®ã€‚")

        if series:
            source_series_lib_names = sorted(list({library_name_map.get(item.get('_SourceLibraryId')) for item in series if item.get('_SourceLibraryId')}))
            logger.info(f"ä»åª’ä½“åº“ã€{', '.join(source_series_lib_names)}ã€‘è·å–åˆ° {len(series)} ä¸ªç”µè§†å‰§é¡¹ç›®ã€‚")

        all_items = movies + series
        total = len(all_items)
        # --- â˜…â˜…â˜… è¡¥å…¨ç»“æŸ â˜…â˜…â˜… ---
        
        if total == 0:
            logger.info("åœ¨æ‰€æœ‰é€‰å®šçš„åº“ä¸­æœªæ‰¾åˆ°ä»»ä½•å¯å¤„ç†çš„é¡¹ç›®ã€‚")
            if update_status_callback: update_status_callback(100, "æœªæ‰¾åˆ°å¯å¤„ç†çš„é¡¹ç›®ã€‚")
            return

        for i, item in enumerate(all_items):
            if self.is_stop_requested(): break
            
            item_id = item.get('Id')
            item_name = item.get('Name', f"ID:{item_id}")

            if not force_reprocess_all and item_id in self.processed_items_cache:
                logger.info(f"æ­£åœ¨è·³è¿‡å·²å¤„ç†çš„é¡¹ç›®: {item_name}")
                if update_status_callback:
                    update_status_callback(int(((i + 1) / total) * 100), f"è·³è¿‡: {item_name}")
                continue

            if update_status_callback:
                update_status_callback(int(((i + 1) / total) * 100), f"å¤„ç†ä¸­ ({i+1}/{total}): {item_name}")
            
            self.process_single_item(
                item_id, 
                force_reprocess_this_item=force_reprocess_all,
                force_fetch_from_tmdb=force_fetch_from_tmdb
            )
            
            time.sleep(float(self.config.get("delay_between_items_sec", 0.5)))
        
        if not self.is_stop_requested() and update_status_callback:
            update_status_callback(100, "å…¨é‡å¤„ç†å®Œæˆ")
    # --- ä¸€é”®ç¿»è¯‘ ---
    def translate_cast_list_for_editing(self, 
                                    cast_list: List[Dict[str, Any]], 
                                    title: Optional[str] = None, 
                                    year: Optional[int] = None,
                                    tmdb_id: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        ã€V13 - è¿”ç’å½’çœŸåŒæ ¸ç‰ˆã€‘ä¸ºæ‰‹åŠ¨ç¼–è¾‘é¡µé¢æä¾›çš„ä¸€é”®ç¿»è¯‘åŠŸèƒ½ã€‚
        æ ¹æ®ç”¨æˆ·é…ç½®ï¼Œæ™ºèƒ½é€‰æ‹©å¸¦å…¨å±€ç¼“å­˜çš„ç¿»è¯‘æ¨¡å¼ï¼Œæˆ–æ— ç¼“å­˜çš„é¡¾é—®æ¨¡å¼ã€‚
        """
        if not cast_list:
            return []
            
        # ä»é…ç½®ä¸­è¯»å–æ¨¡å¼ï¼Œè¿™æ˜¯å†³å®šåç»­æ‰€æœ‰è¡Œä¸ºçš„æ€»å¼€å…³
        translation_mode = self.config.get(constants.CONFIG_OPTION_AI_TRANSLATION_MODE, "fast")
        
        context_log = f" (ä¸Šä¸‹æ–‡: {title} {year})" if title and translation_mode == 'quality' else ""
        logger.info(f"æ‰‹åŠ¨ç¼–è¾‘-ä¸€é”®ç¿»è¯‘ï¼šå¼€å§‹æ‰¹é‡å¤„ç† {len(cast_list)} ä½æ¼”å‘˜ (æ¨¡å¼: {translation_mode}){context_log}ã€‚")
        
        translated_cast = [dict(actor) for actor in cast_list]
        
        # --- æ‰¹é‡ç¿»è¯‘é€»è¾‘ ---
        ai_translation_succeeded = False
        
        if self.ai_translator and self.config.get(constants.CONFIG_OPTION_AI_TRANSLATION_ENABLED, False):
            with get_central_db_connection(self.db_path) as conn:
                cursor = conn.cursor()
                
                translation_cache = {} # æœ¬æ¬¡è¿è¡Œçš„å†…å­˜ç¼“å­˜
                texts_to_translate = set()

                # 1. æ”¶é›†æ‰€æœ‰éœ€è¦ç¿»è¯‘çš„è¯æ¡
                texts_to_collect = set()
                for actor in translated_cast:
                    for field_key in ['name', 'role']:
                        text = actor.get(field_key, '').strip()
                        if field_key == 'role':
                            # æ— è®ºæ˜¯æ¼”å‘˜åè¿˜æ˜¯è§’è‰²åï¼Œéƒ½å…ˆæ¸…æ´—ä¸€éï¼Œç¡®ä¿æ‹¿åˆ°çš„æ˜¯æ ¸å¿ƒæ–‡æœ¬
                            # å¯¹äºæ¼”å‘˜åï¼Œè¿™ä¸ªæ¸…æ´—é€šå¸¸æ— å½±å“ï¼Œä½†å¯¹äºè§’è‰²åè‡³å…³é‡è¦
                            text = utils.clean_character_name_static(text)
                        if text and not utils.contains_chinese(text):
                            texts_to_collect.add(text)

                # 2. æ ¹æ®æ¨¡å¼å†³å®šæ˜¯å¦ä½¿ç”¨ç¼“å­˜
                if translation_mode == 'fast':
                    logger.debug("[ç¿»è¯‘æ¨¡å¼] æ­£åœ¨æ£€æŸ¥å…¨å±€ç¿»è¯‘ç¼“å­˜...")
                    for text in texts_to_collect:
                        # ç¿»è¯‘æ¨¡å¼åªè¯»å†™å…¨å±€ç¼“å­˜
                        cached_entry = self.actor_db_manager.get_translation_from_db(cursor=cursor, text=text)
                        if cached_entry:
                            translation_cache[text] = cached_entry.get("translated_text")
                        else:
                            texts_to_translate.add(text)
                else: # 'quality' mode
                    logger.debug("[é¡¾é—®æ¨¡å¼] è·³è¿‡ç¼“å­˜æ£€æŸ¥ï¼Œç›´æ¥ç¿»è¯‘æ‰€æœ‰è¯æ¡ã€‚")
                    texts_to_translate = texts_to_collect

                # 3. å¦‚æœæœ‰éœ€è¦ç¿»è¯‘çš„è¯æ¡ï¼Œè°ƒç”¨AI
                if texts_to_translate:
                    logger.info(f"æ‰‹åŠ¨ç¼–è¾‘-ç¿»è¯‘ï¼šå°† {len(texts_to_translate)} ä¸ªè¯æ¡æäº¤ç»™AI (æ¨¡å¼: {translation_mode})ã€‚")
                    try:
                        translation_map_from_api = self.ai_translator.batch_translate(
                            texts=list(texts_to_translate),
                            mode=translation_mode,
                            title=title,
                            year=year
                        )
                        if translation_map_from_api:
                            translation_cache.update(translation_map_from_api)
                            
                            # åªæœ‰åœ¨ç¿»è¯‘æ¨¡å¼ä¸‹ï¼Œæ‰å°†ç»“æœå†™å…¥å…¨å±€ç¼“å­˜
                            if translation_mode == 'fast':
                                for original, translated in translation_map_from_api.items():
                                    self.actor_db_manager.save_translation_to_db(
                                        cursor=cursor,
                                        original_text=original, 
                                        translated_text=translated, 
                                        engine_used=self.ai_translator.provider
                                    )
                            
                            ai_translation_succeeded = True
                        else:
                            logger.warning("æ‰‹åŠ¨ç¼–è¾‘-ç¿»è¯‘ï¼šAIæ‰¹é‡ç¿»è¯‘æœªè¿”å›ç»“æœã€‚")
                    except Exception as e:
                        logger.error(f"æ‰‹åŠ¨ç¼–è¾‘-ç¿»è¯‘ï¼šè°ƒç”¨AIæ‰¹é‡ç¿»è¯‘æ—¶å‡ºé”™: {e}", exc_info=True)
                else:
                    logger.info("æ‰‹åŠ¨ç¼–è¾‘-ç¿»è¯‘ï¼šæ‰€æœ‰è¯æ¡å‡åœ¨ç¼“å­˜ä¸­æ‰¾åˆ°ï¼Œæ— éœ€è°ƒç”¨APIã€‚")
                    ai_translation_succeeded = True

                # 4. å›å¡«æ‰€æœ‰ç¿»è¯‘ç»“æœ
                if translation_cache:
                    for i, actor in enumerate(translated_cast):
                        original_name = actor.get('name', '').strip()
                        if original_name in translation_cache:
                            translated_cast[i]['name'] = translation_cache[original_name]
                        
                        original_role_raw = actor.get('role', '').strip()
                        # ä½¿ç”¨ä¸æ”¶é›†æ—¶å®Œå…¨ç›¸åŒçš„æ¸…ç†é€»è¾‘
                        cleaned_original_role = utils.clean_character_name_static(original_role_raw)
                        
                        # ç”¨æ¸…ç†åçš„åå­—ä½œä¸ºkeyå»æŸ¥æ‰¾
                        if cleaned_original_role in translation_cache:
                            translated_cast[i]['role'] = translation_cache[cleaned_original_role]
                        
                        # å¦‚æœå‘ç”Ÿäº†ç¿»è¯‘ï¼Œæ›´æ–°çŠ¶æ€ä»¥ä¾¿å‰ç«¯é«˜äº®
                        if translated_cast[i].get('name') != actor.get('name') or translated_cast[i].get('role') != actor.get('role'):
                            translated_cast[i]['matchStatus'] = 'å·²ç¿»è¯‘'
        
        # å¦‚æœAIç¿»è¯‘æœªå¯ç”¨æˆ–å¤±è´¥ï¼Œåˆ™é™çº§åˆ°ä¼ ç»Ÿå¼•æ“
        if not ai_translation_succeeded:
            if self.config.get("ai_translation_enabled", False):
                logger.info("æ‰‹åŠ¨ç¼–è¾‘-ç¿»è¯‘ï¼šAIç¿»è¯‘å¤±è´¥ï¼Œé™çº§åˆ°ä¼ ç»Ÿå¼•æ“é€ä¸ªç¿»è¯‘ã€‚")
            else:
                logger.info("æ‰‹åŠ¨ç¼–è¾‘-ç¿»è¯‘ï¼šAIæœªå¯ç”¨ï¼Œä½¿ç”¨ä¼ ç»Ÿå¼•æ“é€ä¸ªç¿»è¯‘ã€‚")
                
            try:
                with get_central_db_connection(self.db_path) as conn:
                    cursor = conn.cursor()

                    for i, actor in enumerate(translated_cast):
                        if self.is_stop_requested():
                            logger.warning(f"ä¸€é”®ç¿»è¯‘ï¼ˆé™çº§æ¨¡å¼ï¼‰è¢«ç”¨æˆ·ä¸­æ­¢ã€‚")
                            break # è¿™é‡Œä½¿ç”¨ break æ›´å®‰å…¨ï¼Œå¯ä»¥ç›´æ¥è·³å‡ºå¾ªç¯
                        # ã€ã€ã€ ä¿®å¤ç‚¹ 3ï¼šä½¿ç”¨æ­£ç¡®çš„å‚æ•°è°ƒç”¨ translate_actor_field ã€‘ã€‘ã€‘
                        
                        # ç¿»è¯‘æ¼”å‘˜å
                        name_to_translate = actor.get('name', '').strip()
                        if name_to_translate and not utils.contains_chinese(name_to_translate):
                            translated_name = actor_utils.translate_actor_field(
                                text=name_to_translate,
                                db_manager=self.actor_db_manager,
                                db_cursor=cursor,
                                ai_translator=self.ai_translator,
                                translator_engines=self.translator_engines,
                                ai_enabled=self.ai_enabled
                            )
                            if translated_name and translated_name != name_to_translate:
                                translated_cast[i]['name'] = translated_name

                        # ç¿»è¯‘è§’è‰²å
                        role_to_translate = actor.get('role', '').strip()
                        if role_to_translate and not utils.contains_chinese(role_to_translate):
                            translated_role = actor_utils.translate_actor_field(
                                text=role_to_translate,
                                db_manager=self.actor_db_manager,
                                db_cursor=cursor,
                                ai_translator=self.ai_translator,
                                translator_engines=self.translator_engines,
                                ai_enabled=self.ai_enabled
                            )
                            if translated_role and translated_role != role_to_translate:
                                translated_cast[i]['role'] = translated_role

                        if translated_cast[i].get('name') != actor.get('name') or translated_cast[i].get('role') != actor.get('role'):
                            translated_cast[i]['matchStatus'] = 'å·²ç¿»è¯‘'
            
            except Exception as e:
                logger.error(f"ä¸€é”®ç¿»è¯‘ï¼ˆé™çº§æ¨¡å¼ï¼‰æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)

        logger.info("æ‰‹åŠ¨ç¼–è¾‘-ç¿»è¯‘å®Œæˆã€‚")
        return translated_cast
    # âœ¨âœ¨âœ¨æ‰‹åŠ¨å¤„ç†âœ¨âœ¨âœ¨
    def process_item_with_manual_cast(self, item_id: str, manual_cast_list: List[Dict[str, Any]], item_name: str) -> bool:
        """
        ã€V-API-Direct - ç›´æ¥å†™å…¥æœ€ç»ˆç‰ˆã€‘
        å®Œå…¨ä¿¡ä»»å‰ç«¯æäº¤çš„æ¼”å‘˜åˆ—è¡¨ï¼Œç›´æ¥å°†å…¶è½¬æ¢ä¸º Emby API æ ¼å¼å¹¶æ‰§è¡Œæ›´æ–°ã€‚
        """
        logger.info(f"  -> æ‰‹åŠ¨å¤„ç†æµç¨‹å¯åŠ¨ï¼šItemID: {item_id} ('{item_name}')")
        
        try:
            # æ­¥éª¤ 1: å°†å‰ç«¯æ•°æ®ç›´æ¥è½¬æ¢ä¸º Emby Handler éœ€è¦çš„æ ¼å¼
            # æˆ‘ä»¬å®Œå…¨ä¿¡ä»»å‰ç«¯æäº¤çš„æ•°æ®çš„é¡ºåºå’Œå†…å®¹ã€‚
            logger.debug(f"æ¥æ”¶åˆ°å‰ç«¯æäº¤çš„ {len(manual_cast_list)} ä½æ¼”å‘˜æ•°æ®ï¼Œå¼€å§‹ç›´æ¥è½¬æ¢...")
            cast_for_emby_handler = []
            for actor in manual_cast_list:
                # å‰ç«¯å¿…é¡»æä¾› emby_person_id å’Œ tmdbId
                emby_pid = actor.get("emby_person_id")
                tmdb_pid = actor.get("tmdbId")
                
                # å¢åŠ ä¸€ä¸ªå®‰å…¨æ£€æŸ¥ï¼Œå¦‚æœå…³é”®IDç¼ºå¤±ï¼Œåˆ™è·³è¿‡
                if not emby_pid:
                    logger.warning(f"è·³è¿‡æ¼”å‘˜ '{actor.get('name')}'ï¼Œå› ä¸ºç¼ºå°‘ emby_person_idã€‚")
                    continue

                cast_for_emby_handler.append({
                    "name": actor.get("name"),
                    "character": actor.get("role"),
                    "emby_person_id": emby_pid,
                    "provider_ids": {
                        "Tmdb": tmdb_pid
                        # æœªæ¥å¯ä»¥æ‰©å±•ï¼Œä»å‰ç«¯æ¥æ”¶æ›´å¤šçš„ Provider IDs
                    }
                })

            # ======================================================================
            # æ­¥éª¤ 2: æ‰§è¡Œâ€œä¸¤æ­¥æ›´æ–°â€
            # ======================================================================
            
            # --- æ­¥éª¤ 2.1: å‰ç½®æ›´æ–°æ¼”å‘˜å ---
            logger.info("  -> æ‰‹åŠ¨å¤„ç†ï¼šæ­¥éª¤ 1/2: æ£€æŸ¥å¹¶æ›´æ–°æ¼”å‘˜åå­—...")
            item_details = emby_handler.get_emby_item_details(item_id, self.emby_url, self.emby_api_key, self.emby_user_id)
            if not item_details:
                raise ValueError(f"æ— æ³•è·å–é¡¹ç›® {item_id} çš„è¯¦æƒ…ã€‚")
                
            original_names_map = {p.get("Id"): p.get("Name") for p in item_details.get("People", []) if p.get("Id")}
            
            for actor in cast_for_emby_handler:
                actor_id = actor.get("emby_person_id")
                new_name = actor.get("name")
                original_name = original_names_map.get(actor_id)
                
                if actor_id and new_name and original_name and new_name != original_name:
                    logger.info(f"  -> æ£€æµ‹åˆ°æ‰‹åŠ¨åå­—å˜æ›´ï¼Œæ­£åœ¨æ›´æ–° Person: '{original_name}' -> '{new_name}' (ID: {actor_id})")
                    emby_handler.update_person_details(
                        person_id=actor_id,
                        new_data={"Name": new_name},
                        emby_server_url=self.emby_url,
                        emby_api_key=self.emby_api_key,
                        user_id=self.emby_user_id
                    )
            logger.info("  -> æ‰‹åŠ¨å¤„ç†ï¼šæ¼”å‘˜åå­—å‰ç½®æ›´æ–°å®Œæˆã€‚")

            # --- æ­¥éª¤ 2.2: æ›´æ–°åª’ä½“çš„æ¼”å‘˜åˆ—è¡¨ ---
            logger.info(f"  -> æ‰‹åŠ¨å¤„ç†ï¼šæ­¥éª¤ 2/2: å‡†å¤‡å°† {len(cast_for_emby_handler)} ä½æ¼”å‘˜æ›´æ–°åˆ°åª’ä½“é¡¹ç›®...")
            update_success = emby_handler.update_emby_item_cast(
                item_id=item_id,
                new_cast_list_for_handler=cast_for_emby_handler,
                emby_server_url=self.emby_url,
                emby_api_key=self.emby_api_key,
                user_id=self.emby_user_id
            )

            if not update_success:
                logger.error(f"  -> æ‰‹åŠ¨å¤„ç†å¤±è´¥ï¼šæ›´æ–° Emby é¡¹ç›® '{item_name}' æ¼”å‘˜ä¿¡æ¯æ—¶å¤±è´¥ã€‚")
                # è®°å½•åˆ°å¤±è´¥æ—¥å¿—
                with get_central_db_connection(self.db_path) as conn:
                    self.log_db_manager.save_to_failed_log(conn.cursor(), item_id, item_name, "æ‰‹åŠ¨APIæ›´æ–°æ¼”å‘˜ä¿¡æ¯å¤±è´¥", item_details.get("Type"))
                return False

            # ======================================================================
            # æ­¥éª¤ 3: è°ƒç”¨â€œé”åŒ â€å®Œæˆä¸Šé”å’Œåˆ·æ–°
            # ======================================================================
            logger.info("  -> æ‰‹åŠ¨æ›´æ–°æˆåŠŸ")
            
            # æ‰‹åŠ¨å¤„ç†åï¼Œæˆ‘ä»¬æ€»æ˜¯å¸Œæœ›é”å®šç»“æœ
            fields_to_lock = ["Cast"] if self.auto_lock_cast_enabled else None
            
            emby_handler.refresh_emby_item_metadata(
                item_emby_id=item_id,
                emby_server_url=self.emby_url,
                emby_api_key=self.emby_api_key,
                user_id_for_ops=self.emby_user_id,
                lock_fields=fields_to_lock,
                replace_all_metadata_param=False, # æ‰‹åŠ¨APIæ›´æ–°åï¼Œè½»é‡çº§åˆ·æ–°å³å¯
                item_name_for_log=item_name
            )

            # ======================================================================
            # æ­¥éª¤ 4: æ›´æ–°å¤„ç†æ—¥å¿—
            # ======================================================================
            with get_central_db_connection(self.db_path) as conn:
                cursor = conn.cursor()
                self.log_db_manager.save_to_processed_log(cursor, item_id, item_name, score=10.0) # æ‰‹åŠ¨å¤„ç†ç›´æ¥ç»™æ»¡åˆ†
                self.log_db_manager.remove_from_failed_log(cursor, item_id)

            logger.info(f"  -> æ‰‹åŠ¨å¤„ç† '{item_name}' æµç¨‹å®Œæˆã€‚")
            return True

        except Exception as e:
            logger.error(f"  -> æ‰‹åŠ¨å¤„ç† '{item_name}' æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
            return False
        finally:
            # æ¸…ç†å†…å­˜ç¼“å­˜ï¼Œæ— è®ºæˆåŠŸå¤±è´¥
            if item_id in self.manual_edit_cache:
                del self.manual_edit_cache[item_id]
                logger.trace(f"å·²æ¸…ç† ItemID {item_id} çš„æ‰‹åŠ¨ç¼–è¾‘ä¼šè¯ç¼“å­˜ã€‚")
    # --- ä¸ºå‰ç«¯å‡†å¤‡æ¼”å‘˜åˆ—è¡¨ç”¨äºç¼–è¾‘ ---
    def get_cast_for_editing(self, item_id: str) -> Optional[Dict[str, Any]]:
        """
        ã€V-API-Optimized - æ€§èƒ½ä¸å±•ç¤ºä¼˜åŒ–æœ€ç»ˆç‰ˆã€‘
        1. æ¼”å‘˜å¤´åƒç›´æ¥ä»æœ¬åœ°æ•°æ®åº“ç¼“å­˜çš„ TMDB è·¯å¾„æ‹¼æ¥ï¼ŒåŠ è½½é€Ÿåº¦æå¿«ã€‚
        2. è§’è‰²ååœ¨è¿”å›ç»™å‰ç«¯å‰è¿›è¡Œæ¸…ç†ï¼Œå»é™¤â€œé¥° â€ç­‰å‰ç¼€ã€‚
        """
        logger.info(f"  -> ä¸ºç¼–è¾‘é¡µé¢å‡†å¤‡æ•°æ®ï¼šItemID {item_id}")
        
        try:
            # æ­¥éª¤ 1: è·å– Emby åŸºç¡€è¯¦æƒ… (ä¿æŒä¸å˜)
            emby_details = emby_handler.get_emby_item_details(item_id, self.emby_url, self.emby_api_key, self.emby_user_id)
            if not emby_details:
                raise ValueError(f"åœ¨Embyä¸­æœªæ‰¾åˆ°é¡¹ç›® {item_id}")

            item_name_for_log = emby_details.get("Name", f"æœªçŸ¥(ID:{item_id})")
            
            # æ­¥éª¤ 2: è·å–æ¼”å‘˜åˆ—è¡¨ (ä¿æŒä¸å˜)
            logger.debug(f"  -> æ­£åœ¨ä¸º '{item_name_for_log}' è·å–æ¼”å‘˜åˆ—è¡¨...")
            raw_emby_people = emby_details.get("People", [])
            full_cast_enhanced = self._enrich_cast_from_db_and_api(raw_emby_people)
            
            if not full_cast_enhanced:
                logger.warning(f"é¡¹ç›® '{item_name_for_log}' æ²¡æœ‰æ¼”å‘˜ä¿¡æ¯å¤±è´¥ã€‚")

            # æ­¥éª¤ 3: ç¼“å­˜å®Œæ•´æ•°æ® (ä¿æŒä¸å˜)
            cast_for_cache = []
            for actor in full_cast_enhanced:
                actor_copy = actor.copy()
                actor_copy['id'] = actor.get("ProviderIds", {}).get("Tmdb")
                actor_copy['emby_person_id'] = actor.get("Id")
                actor_copy['name'] = actor.get("Name")
                actor_copy['character'] = actor.get("Role")
                cast_for_cache.append(actor_copy)
            self.manual_edit_cache[item_id] = cast_for_cache
            logger.debug(f"å·²ä¸º ItemID {item_id} ç¼“å­˜äº† {len(cast_for_cache)} æ¡å®Œæ•´æ¼”å‘˜æ•°æ®ã€‚")

            # â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜… æ­¥éª¤ 4: æ„å»ºå‰ç«¯æ•°æ® (å…¨æ–°ä¼˜åŒ–) â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…
            cast_for_frontend = []
            
            # ä¸ºäº†ä»æ•°æ®åº“è·å–å¤´åƒè·¯å¾„ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ•°æ®åº“è¿æ¥
            with get_central_db_connection(self.db_path) as conn:
                cursor = conn.cursor()
                
                for actor_data in cast_for_cache:
                    tmdb_id = actor_data.get('id')
                    image_url = None
                    
                    # --- æ ¸å¿ƒä¼˜åŒ– 1: ä»æ•°æ®åº“è·å–å¤´åƒè·¯å¾„ ---
                    if tmdb_id:
                        # è°ƒç”¨æˆ‘ä»¬å·²æœ‰çš„è¾…åŠ©å‡½æ•°
                        actor_metadata = self._get_actor_metadata_from_cache(tmdb_id, cursor)
                        if actor_metadata and actor_metadata.get("profile_path"):
                            profile_path = actor_metadata["profile_path"]
                            # æ‹¼æ¥ TMDB å°å°ºå¯¸å¤´åƒ URLï¼ŒåŠ è½½é€Ÿåº¦é£å¿«
                            image_url = f"https://image.tmdb.org/t/p/w185{profile_path}"
                    
                    # --- æ ¸å¿ƒä¼˜åŒ– 2: æ¸…ç†è§’è‰²å ---
                    original_role = actor_data.get('character', '')
                    cleaned_role_for_display = utils.clean_character_name_static(original_role)

                    cast_for_frontend.append({
                        "tmdbId": tmdb_id,
                        "name": actor_data.get('name'),
                        "role": cleaned_role_for_display, # â˜… ä½¿ç”¨æ¸…ç†åçš„è§’è‰²å
                        "imageUrl": image_url,             # â˜… ä½¿ç”¨æ‹¼æ¥çš„ TMDB å¤´åƒ URL
                        "emby_person_id": actor_data.get('emby_person_id')
                    })
            
            # æ­¥éª¤ 5: å‡†å¤‡å¹¶è¿”å›æœ€ç»ˆçš„å“åº”æ•°æ® (ä¿æŒä¸å˜)
            failed_log_info = {}
            with get_central_db_connection(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT error_message, score FROM failed_log WHERE item_id = ?", (item_id,))
                row = cursor.fetchone()
                if row: failed_log_info = dict(row)

            response_data = {
                "item_id": item_id,
                "item_name": emby_details.get("Name"),
                "item_type": emby_details.get("Type"),
                "image_tag": emby_details.get('ImageTags', {}).get('Primary'),
                "original_score": failed_log_info.get("score"),
                "review_reason": failed_log_info.get("error_message"),
                "current_emby_cast": cast_for_frontend,
                "search_links": {
                    "google_search_wiki": utils.generate_search_url('wikipedia', emby_details.get("Name"), emby_details.get("ProductionYear"))
                }
            }
            return response_data

        except Exception as e:
            logger.error(f"  -> è·å–ç¼–è¾‘æ•°æ®å¤±è´¥ for ItemID {item_id}: {e}", exc_info=True)
            return None
    
    # â˜…â˜…â˜… å…¨é‡å¤‡ä»½åˆ°è¦†ç›–ç¼“å­˜ â˜…â˜…â˜…
    def sync_all_media_assets(self, update_status_callback: Optional[callable] = None):
        """
        ã€V-Final Migration Tool - æœ€ç»ˆè¿ç§»ç‰ˆã€‘
        éå†æ‰€æœ‰å·²å¤„ç†çš„åª’ä½“é¡¹ï¼Œæ‰§è¡Œä¸¤å¤§ä»»åŠ¡ï¼š
        1. å°† Emby ä¸­çš„å½“å‰å›¾ç‰‡ä¸‹è½½åˆ°æœ¬åœ° override/images ç›®å½•ã€‚
        2. å°† cache ç›®å½•ä¸­çš„ JSON å…ƒæ•°æ®æ–‡ä»¶å¤åˆ¶åˆ° override ç›®å½•ã€‚
        """
        task_name = "å…¨é‡åŒæ­¥åª’ä½“èµ„äº§ (å›¾ç‰‡+å…ƒæ•°æ®)"
        logger.info(f"--- å¼€å§‹æ‰§è¡Œ '{task_name}' ä»»åŠ¡ ---")
        
        # æ£€æŸ¥æœ¬åœ°æ•°æ®è·¯å¾„æ˜¯å¦å·²é…ç½®ï¼Œè¿™æ˜¯æ­¤åŠŸèƒ½çš„åŸºç¡€
        if not self.local_data_path:
            logger.error(f"'{task_name}' å¤±è´¥ï¼šæœªåœ¨é…ç½®ä¸­è®¾ç½®â€œæœ¬åœ°æ•°æ®æºè·¯å¾„â€ã€‚")
            if update_status_callback:
                update_status_callback(-1, "æœªé…ç½®æœ¬åœ°æ•°æ®æºè·¯å¾„")
            return

        try:
            with get_central_db_connection(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT item_id, item_name FROM processed_log")
                items_to_process = cursor.fetchall()
        except Exception as e:
            logger.error(f"è·å–å·²å¤„ç†é¡¹ç›®åˆ—è¡¨æ—¶å‘ç”Ÿæ•°æ®åº“é”™è¯¯: {e}", exc_info=True)
            if update_status_callback:
                update_status_callback(-1, "æ•°æ®åº“é”™è¯¯")
            return

        total = len(items_to_process)
        if total == 0:
            logger.info("æ²¡æœ‰å·²å¤„ç†çš„é¡¹ç›®ï¼Œæ— éœ€åŒæ­¥ã€‚")
            if update_status_callback:
                update_status_callback(100, "æ²¡æœ‰é¡¹ç›®")
            return

        logger.info(f"å…±æ‰¾åˆ° {total} ä¸ªå·²å¤„ç†é¡¹ç›®éœ€è¦åŒæ­¥èµ„äº§ã€‚")
        
        stats = {"images_synced": 0, "metadata_copied": 0, "skipped": 0}

        for i, db_row in enumerate(items_to_process):
            if self.is_stop_requested():
                logger.info(f"'{task_name}' ä»»åŠ¡è¢«ä¸­æ­¢ã€‚")
                break

            item_id = db_row['item_id']
            item_name_from_db = db_row['item_name']
            
            if update_status_callback:
                update_status_callback(int((i / total) * 100), f"({i+1}/{total}): {item_name_from_db}")

            try:
                item_details = emby_handler.get_emby_item_details(item_id, self.emby_url, self.emby_api_key, self.emby_user_id)
                if not item_details:
                    logger.warning(f"è·³è¿‡ '{item_name_from_db}' (ID: {item_id})ï¼Œæ— æ³•ä» Emby è·å–å…¶è¯¦æƒ…ã€‚")
                    stats["skipped"] += 1
                    continue

                tmdb_id = item_details.get("ProviderIds", {}).get("Tmdb")
                item_type = item_details.get("Type")
                if not tmdb_id:
                    logger.warning(f"è·³è¿‡ '{item_name_from_db}'ï¼Œå› ä¸ºå®ƒç¼ºå°‘ TMDb IDã€‚")
                    stats["skipped"] += 1
                    continue

                # ======================================================================
                # â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜… ä»»åŠ¡ä¸€ï¼šåŒæ­¥å›¾ç‰‡ â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…
                # ======================================================================
                # (è¿™éƒ¨åˆ†é€»è¾‘ç›´æ¥è°ƒç”¨æˆ‘ä»¬ä¹‹å‰é‡æ„å¥½çš„ sync_item_images å‡½æ•°ï¼Œéå¸¸ä¼˜é›…)
                logger.info(f"  -> æ­£åœ¨ä¸º '{item_name_from_db}' åŒæ­¥å›¾ç‰‡...")
                self.sync_item_images(item_details)
                stats["images_synced"] += 1
                
                # ======================================================================
                # â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜… ä»»åŠ¡äºŒï¼šå¤åˆ¶å…ƒæ•°æ® â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…
                # ======================================================================
                logger.info(f"  -> æ­£åœ¨ä¸º '{item_name_from_db}' è¿ç§»å…ƒæ•°æ®...")
                
                cache_folder_name = "tmdb-movies2" if item_type == "Movie" else "tmdb-tv"
                source_cache_dir = os.path.join(self.local_data_path, "cache", cache_folder_name, tmdb_id)
                target_override_dir = os.path.join(self.local_data_path, "override", cache_folder_name, tmdb_id)

                if os.path.exists(source_cache_dir):
                    try:
                        # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
                        os.makedirs(target_override_dir, exist_ok=True)
                        
                        # ä½¿ç”¨ shutil.copytree æ¥é€’å½’å¤åˆ¶æ•´ä¸ªç›®å½•
                        # dirs_exist_ok=True (Python 3.8+) å¯ä»¥è¦†ç›–ç°æœ‰æ–‡ä»¶
                        shutil.copytree(source_cache_dir, target_override_dir, dirs_exist_ok=True)
                        
                        logger.info(f"    âœ… æˆåŠŸå°†å…ƒæ•°æ®ä» '{source_cache_dir}' è¿ç§»åˆ° '{target_override_dir}'ã€‚")
                        stats["metadata_copied"] += 1
                    except Exception as e_copy:
                        logger.error(f"    âŒ è¿ç§»å…ƒæ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e_copy}", exc_info=True)
                else:
                    logger.debug(f"    - è·³è¿‡å…ƒæ•°æ®è¿ç§»ï¼Œå› ä¸ºæºç¼“å­˜ç›®å½•ä¸å­˜åœ¨: {source_cache_dir}")

            except Exception as e:
                logger.error(f"å¤„ç†é¡¹ç›® '{item_name_from_db}' (ID: {item_id}) æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
                stats["skipped"] += 1
            
            time.sleep(0.1) # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹

        logger.info("--- å…¨é‡åª’ä½“èµ„äº§åŒæ­¥ä»»åŠ¡ç»“æŸ ---")
        final_message = f"åŒæ­¥å®Œæˆï¼å›¾ç‰‡: {stats['images_synced']}, å…ƒæ•°æ®: {stats['metadata_copied']}, è·³è¿‡: {stats['skipped']}ã€‚"
        logger.info(final_message)
        if update_status_callback:
            update_status_callback(100, final_message)
   
    # --- å›¾ç‰‡åŒæ­¥ ---
    def sync_item_images(self, item_details: Dict[str, Any], update_description: Optional[str] = None) -> bool:
        """
        ã€æ–°å¢-é‡æ„ã€‘è¿™ä¸ªæ–¹æ³•è´Ÿè´£åŒæ­¥ä¸€ä¸ªåª’ä½“é¡¹ç›®çš„æ‰€æœ‰ç›¸å…³å›¾ç‰‡ã€‚
        å®ƒä» _process_item_core_logic ä¸­æå–å‡ºæ¥ï¼Œä»¥ä¾¿å¤ç”¨ã€‚
        """
        item_id = item_details.get("Id")
        item_type = item_details.get("Type")
        item_name_for_log = item_details.get("Name", f"æœªçŸ¥é¡¹ç›®(ID:{item_id})")
        
        if not all([item_id, item_type, self.local_data_path]):
            logger.error(f"[å›¾ç‰‡åŒæ­¥] è·³è¿‡ '{item_name_for_log}'ï¼Œå› ä¸ºç¼ºå°‘IDã€ç±»å‹æˆ–æœªé…ç½®æœ¬åœ°æ•°æ®è·¯å¾„ã€‚")
            return False

        try:
            # --- å‡†å¤‡å·¥ä½œ (ç›®å½•ã€TMDb IDç­‰) ---
            log_prefix = "[å›¾ç‰‡åŒæ­¥]"
            tmdb_id = item_details.get("ProviderIds", {}).get("Tmdb")
            if not tmdb_id:
                logger.warning(f"{log_prefix} é¡¹ç›® '{item_name_for_log}' ç¼ºå°‘TMDb IDï¼Œæ— æ³•ç¡®å®šè¦†ç›–ç›®å½•ï¼Œè·³è¿‡ã€‚")
                return False
            
            cache_folder_name = "tmdb-movies2" if item_type == "Movie" else "tmdb-tv"
            base_override_dir = os.path.join(self.local_data_path, "override", cache_folder_name, tmdb_id)
            image_override_dir = os.path.join(base_override_dir, "images")
            os.makedirs(image_override_dir, exist_ok=True)

            # --- å®šä¹‰æ‰€æœ‰å¯èƒ½çš„å›¾ç‰‡æ˜ å°„ ---
            full_image_map = {"Primary": "poster.jpg", "Backdrop": "fanart.jpg", "Logo": "clearlogo.png"}
            if item_type == "Movie":
                full_image_map["Thumb"] = "landscape.jpg"

            # â˜…â˜…â˜… å…¨æ–°é€»è¾‘åˆ†å‘ â˜…â˜…â˜…
            images_to_sync = {}
            
            # æ¨¡å¼ä¸€ï¼šç²¾å‡†åŒæ­¥ (å½“æè¿°å­˜åœ¨æ—¶)
            if update_description:
                log_prefix = "[ç²¾å‡†å›¾ç‰‡åŒæ­¥]"
                logger.debug(f"{log_prefix} æ­£åœ¨è§£ææè¿°: '{update_description}'")
                
                # å®šä¹‰å…³é”®è¯åˆ°Embyå›¾ç‰‡ç±»å‹çš„æ˜ å°„ (ä½¿ç”¨å°å†™ä»¥æ–¹ä¾¿åŒ¹é…)
                keyword_map = {
                    "primary": "Primary",
                    "backdrop": "Backdrop",
                    "logo": "Logo",
                    "thumb": "Thumb", # ç”µå½±ç¼©ç•¥å›¾
                    "banner": "Banner" # å‰§é›†æ¨ªå¹… (å¦‚æœéœ€è¦å¯ä»¥æ·»åŠ )
                }
                
                desc_lower = update_description.lower()
                found_specific_image = False
                for keyword, image_type_api in keyword_map.items():
                    if keyword in desc_lower and image_type_api in full_image_map:
                        images_to_sync[image_type_api] = full_image_map[image_type_api]
                        logger.debug(f"{log_prefix} åŒ¹é…åˆ°å…³é”®è¯ '{keyword}'ï¼Œå°†åªåŒæ­¥ {image_type_api} å›¾ç‰‡ã€‚")
                        found_specific_image = True
                        break # æ‰¾åˆ°ç¬¬ä¸€ä¸ªåŒ¹é…å°±åœæ­¢ï¼Œé¿å…é‡å¤
                
                if not found_specific_image:
                    logger.warning(f"{log_prefix} æœªèƒ½åœ¨æè¿°ä¸­æ‰¾åˆ°å¯è¯†åˆ«çš„å›¾ç‰‡å…³é”®è¯ï¼Œå°†å›é€€åˆ°å®Œå…¨åŒæ­¥ã€‚")
                    images_to_sync = full_image_map # å›é€€
            
            # æ¨¡å¼äºŒï¼šå®Œå…¨åŒæ­¥ (é»˜è®¤æˆ–å›é€€)
            else:
                log_prefix = "[å®Œæ•´å›¾ç‰‡åŒæ­¥]"
                logger.debug(f"{log_prefix} æœªæä¾›æ›´æ–°æè¿°ï¼Œå°†åŒæ­¥æ‰€æœ‰ç±»å‹çš„å›¾ç‰‡ã€‚")
                images_to_sync = full_image_map

            # --- æ‰§è¡Œä¸‹è½½ ---
            logger.info(f"{log_prefix} å¼€å§‹ä¸º '{item_name_for_log}' ä¸‹è½½ {len(images_to_sync)} å¼ å›¾ç‰‡è‡³ {image_override_dir}...")
            for image_type, filename in images_to_sync.items():
                if self.is_stop_requested():
                    logger.warning(f"{log_prefix} æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œä¸­æ­¢å›¾ç‰‡ä¸‹è½½ã€‚")
                    return False
                emby_handler.download_emby_image(item_id, image_type, os.path.join(image_override_dir, filename), self.emby_url, self.emby_api_key)
            
            # --- åˆ†é›†å›¾ç‰‡é€»è¾‘ (åªæœ‰åœ¨å®Œå…¨åŒæ­¥æ—¶æ‰è€ƒè™‘æ‰§è¡Œ) ---
            if images_to_sync == full_image_map and item_type == "Series":
            
                children = emby_handler.get_series_children(item_id, self.emby_url, self.emby_api_key, self.emby_user_id, series_name_for_log=item_name_for_log) or []
                for child in children:
                    if self.is_stop_requested():
                        logger.warning(f"{log_prefix} æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œä¸­æ­¢å­é¡¹ç›®å›¾ç‰‡ä¸‹è½½ã€‚")
                        return False
                    child_type, child_id = child.get("Type"), child.get("Id")
                    if child_type == "Season":
                        season_number = child.get("IndexNumber")
                        if season_number is not None:
                            emby_handler.download_emby_image(child_id, "Primary", os.path.join(image_override_dir, f"season-{season_number}.jpg"), self.emby_url, self.emby_api_key)
                    elif child_type == "Episode":
                        season_number, episode_number = child.get("ParentIndexNumber"), child.get("IndexNumber")
                        if season_number is not None and episode_number is not None:
                            emby_handler.download_emby_image(child_id, "Primary", os.path.join(image_override_dir, f"season-{season_number}-episode-{episode_number}.jpg"), self.emby_url, self.emby_api_key)
            
            logger.info(f"{log_prefix} âœ… æˆåŠŸå®Œæˆ '{item_name_for_log}' çš„å›¾ç‰‡åŒæ­¥ã€‚")
            return True
        except Exception as e:
            logger.error(f"{log_prefix} ä¸º '{item_name_for_log}' åŒæ­¥å›¾ç‰‡æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
            return False
    
    def close(self):
        if self.douban_api: self.douban_api.close()
