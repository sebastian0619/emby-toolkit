# web_app.py
from gevent import monkey
monkey.patch_all()
import os
import sys
import shutil
import threading
from datetime import datetime, timezone # Added timezone for image.update
from jinja2 import Environment, FileSystemLoader
from actor_sync_handler import UnifiedSyncHandler
from db_handler import ActorDBManager
import emby_handler
import moviepilot_handler
import utils
from tasks import *
import extensions
from extensions import (
    login_required, 
    task_lock_required, 
    processor_ready_required
)
from utils import LogDBManager
from flask import Flask, render_template, request, redirect, url_for, jsonify, flash, stream_with_context, send_from_directory,Response, abort, session
from werkzeug.utils import safe_join, secure_filename
from utils import get_override_path_for_item
from watchlist_processor import WatchlistProcessor
from datetime import datetime
import requests
import tmdb_handler
import task_manager
from douban import DoubanApi
from tasks import get_task_registry 
from typing import Optional, Dict, Any, List, Tuple, Union # ç¡®ä¿ List è¢«å¯¼å…¥
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
import pytz # ç”¨äºå¤„ç†æ—¶åŒº
import atexit # ç”¨äºåº”ç”¨é€€å‡ºå¤„ç†
from core_processor import MediaProcessor
from actor_subscription_processor import ActorSubscriptionProcessor
from werkzeug.security import generate_password_hash, check_password_hash
from actor_utils import enrich_all_actor_aliases_task
import db_handler
from db_handler import get_db_connection as get_central_db_connection
from flask import session
from croniter import croniter
from scheduler_manager import scheduler_manager
from reverse_proxy import proxy_app
import logging
import collections # Added for deque
from gevent import spawn_later # Added for debouncing
# --- å¯¼å…¥è“å›¾ ---
from routes.watchlist import watchlist_bp
from routes.collections import collections_bp
from routes.custom_collections import custom_collections_bp
from routes.actor_subscriptions import actor_subscriptions_bp
from routes.logs import logs_bp
from routes.database_admin import db_admin_bp
from routes.system import system_bp
from routes.media import media_api_bp, media_proxy_bp
from routes.auth import auth_bp, init_auth as init_auth_from_blueprint
from routes.actions import actions_bp
from routes.cover_generator_config import cover_generator_config_bp
from routes.tasks import tasks_bp
from routes.resubscribe import resubscribe_bp
# --- æ ¸å¿ƒæ¨¡å—å¯¼å…¥ ---
import constants # ä½ çš„å¸¸é‡å®šä¹‰\
import logging
from logger_setup import frontend_log_queue, add_file_handler # æ—¥å¿—è®°å½•å™¨å’Œå‰ç«¯æ—¥å¿—é˜Ÿåˆ—
import utils       # ä¾‹å¦‚ï¼Œç”¨äº /api/search_media
import config_manager

import task_manager
# --- æ ¸å¿ƒæ¨¡å—å¯¼å…¥ç»“æŸ ---
logger = logging.getLogger(__name__)
logging.getLogger("apscheduler.scheduler").setLevel(logging.WARNING)
logging.getLogger('werkzeug').setLevel(logging.ERROR)
app = Flask(__name__, static_folder='static')
app.secret_key = os.urandom(24)

#è¿‡æ»¤åº•å±‚æ—¥å¿—
logging.getLogger("requests").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("httpcore").setLevel(logging.WARNING)
logging.getLogger("openai").setLevel(logging.WARNING)
logging.getLogger("docker").setLevel(logging.WARNING)
logging.getLogger("PIL").setLevel(logging.WARNING)
logging.getLogger("geventwebsocket").setLevel(logging.WARNING)
# --- å…¨å±€å˜é‡ ---

JOB_ID_FULL_SCAN = "scheduled_full_scan"
JOB_ID_SYNC_PERSON_MAP = "scheduled_sync_person_map"
JOB_ID_PROCESS_WATCHLIST = "scheduled_process_watchlist"
JOB_ID_REVIVAL_CHECK = "scheduled_revival_check"

# Webhook æ‰¹é‡å¤„ç†ç›¸å…³
WEBHOOK_BATCH_QUEUE = collections.deque()
WEBHOOK_BATCH_LOCK = threading.Lock()
WEBHOOK_BATCH_DEBOUNCE_TIME = 5 # ç§’ï¼Œåœ¨æ­¤æ—¶é—´å†…æ”¶é›†äº‹ä»¶
WEBHOOK_BATCH_DEBOUNCER = None

# â˜…â˜…â˜… ä¸º metadata/image update äº‹ä»¶å¢åŠ é˜²æŠ–æœºåˆ¶ â˜…â˜…â˜…
UPDATE_DEBOUNCE_TIMERS = {}
UPDATE_DEBOUNCE_LOCK = threading.Lock()
UPDATE_DEBOUNCE_TIME = 15 # ç§’ï¼Œç­‰å¾…äº‹ä»¶é£æš´ç»“æŸ

# --- æ•°æ®åº“è¾…åŠ©å‡½æ•° ---
def task_process_single_item(processor: MediaProcessor, item_id: str, force_reprocess: bool):
    """ä»»åŠ¡ï¼šå¤„ç†å•ä¸ªåª’ä½“é¡¹"""
    processor.process_single_item(item_id, force_reprocess)

# --- åˆå§‹åŒ–æ•°æ®åº“ ---
def init_db():
    """
    ã€PostgreSQLç‰ˆã€‘åˆå§‹åŒ–æ•°æ®åº“ï¼Œåˆ›å»ºæ‰€æœ‰è¡¨çš„æœ€ç»ˆç»“æ„ã€‚
    """
    logger.info("æ­£åœ¨åˆå§‹åŒ– PostgreSQL æ•°æ®åº“ï¼Œåˆ›å»º/éªŒè¯æ‰€æœ‰è¡¨çš„ç»“æ„...")
    
    # get_central_db_connection åº”è¯¥å°±æ˜¯ db_handler.get_db_connection
    # ç¡®ä¿å®ƒç°åœ¨è°ƒç”¨çš„æ˜¯æ— å‚æ•°ç‰ˆæœ¬
    try:
        with db_handler.get_db_connection() as conn:
            with conn.cursor() as cursor:
                logger.info("  -> æ•°æ®åº“è¿æ¥æˆåŠŸï¼Œå¼€å§‹å»ºè¡¨...")

                # --- 1. åˆ›å»ºåŸºç¡€è¡¨ (æ—¥å¿—ã€ç¼“å­˜ã€ç”¨æˆ·) ---
                logger.trace("  -> æ­£åœ¨åˆ›å»ºåŸºç¡€è¡¨...")
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS processed_log (
                        item_id TEXT PRIMARY KEY, 
                        item_name TEXT, 
                        processed_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(), 
                        score REAL,
                        assets_synced_at TIMESTAMP WITH TIME ZONE,
                        last_emby_modified_at TIMESTAMP WITH TIME ZONE
                    )
                """)
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS failed_log (
                        item_id TEXT PRIMARY KEY, 
                        item_name TEXT, 
                        reason TEXT, 
                        failed_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(), 
                        error_message TEXT, 
                        item_type TEXT, 
                        score REAL
                    )
                """)
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS users (
                        id SERIAL PRIMARY KEY, 
                        username TEXT UNIQUE NOT NULL, 
                        password_hash TEXT NOT NULL, 
                        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
                    )
                """)
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS translation_cache (
                        original_text TEXT PRIMARY KEY, 
                        translated_text TEXT, 
                        engine_used TEXT, 
                        last_updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
                    )
                """)

                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS app_settings (
                        setting_key TEXT PRIMARY KEY,
                        value_json JSONB,
                        last_updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
                    )
                """)

                # --- 2. åˆ›å»ºæ ¸å¿ƒåŠŸèƒ½è¡¨ ---
                logger.trace("  -> æ­£åœ¨åˆ›å»º 'collections_info' è¡¨...")
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS collections_info (
                        emby_collection_id TEXT PRIMARY KEY,
                        name TEXT,
                        tmdb_collection_id TEXT,
                        status TEXT,
                        has_missing BOOLEAN, 
                        missing_movies_json JSONB,
                        last_checked_at TIMESTAMP WITH TIME ZONE,
                        poster_path TEXT,
                        item_type TEXT DEFAULT 'Movie' NOT NULL,
                        in_library_count INTEGER DEFAULT 0
                    )
                """)

                logger.trace("  -> æ­£åœ¨åˆ›å»º 'custom_collections' è¡¨...")
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS custom_collections (
                        id SERIAL PRIMARY KEY,
                        name TEXT NOT NULL UNIQUE,
                        type TEXT NOT NULL,
                        definition_json JSONB NOT NULL,
                        status TEXT DEFAULT 'active',
                        emby_collection_id TEXT,
                        last_synced_at TIMESTAMP WITH TIME ZONE,
                        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                        health_status TEXT,
                        item_type TEXT,
                        in_library_count INTEGER DEFAULT 0,
                        missing_count INTEGER DEFAULT 0,
                        generated_media_info_json JSONB,
                        poster_path TEXT,
                        sort_order INTEGER NOT NULL DEFAULT 0
                    )
                """)
                cursor.execute("CREATE INDEX IF NOT EXISTS idx_cc_type ON custom_collections (type)")
                cursor.execute("CREATE INDEX IF NOT EXISTS idx_cc_status ON custom_collections (status)")

                logger.trace("  -> æ­£åœ¨åˆ›å»º 'media_metadata' è¡¨...")
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS media_metadata (
                        tmdb_id TEXT,
                        item_type TEXT NOT NULL,
                        title TEXT,
                        original_title TEXT,
                        release_year INTEGER,
                        rating REAL,
                        genres_json JSONB,
                        actors_json JSONB,
                        directors_json JSONB,
                        studios_json JSONB,
                        countries_json JSONB,
                        last_updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                        release_date DATE,
                        date_added TIMESTAMP WITH TIME ZONE,
                        tags_json JSONB,
                        last_synced_at TIMESTAMP WITH TIME ZONE,
                        PRIMARY KEY (tmdb_id, item_type)
                    )
                """)

                logger.trace("  -> æ­£åœ¨åˆ›å»º 'watchlist' è¡¨...")
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS watchlist (
                        item_id TEXT PRIMARY KEY,
                        tmdb_id TEXT NOT NULL,
                        item_name TEXT,
                        item_type TEXT DEFAULT 'Series',
                        status TEXT DEFAULT 'Watching',
                        added_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                        last_checked_at TIMESTAMP WITH TIME ZONE,
                        tmdb_status TEXT,
                        next_episode_to_air_json JSONB,
                        missing_info_json JSONB,
                        paused_until DATE DEFAULT NULL,
                        force_ended BOOLEAN DEFAULT FALSE NOT NULL
                    )
                """)
                cursor.execute("CREATE INDEX IF NOT EXISTS idx_watchlist_status ON watchlist (status)")

                logger.trace("  -> æ­£åœ¨åˆ›å»º 'person_identity_map' è¡¨...")
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS person_identity_map (
                        map_id SERIAL PRIMARY KEY, 
                        primary_name TEXT NOT NULL, 
                        emby_person_id TEXT NOT NULL UNIQUE,
                        tmdb_person_id INTEGER UNIQUE, 
                        imdb_id TEXT UNIQUE, 
                        douban_celebrity_id TEXT UNIQUE,
                        last_synced_at TIMESTAMP WITH TIME ZONE, 
                        last_updated_at TIMESTAMP WITH TIME ZONE
                    )
                """)

                logger.trace("  -> æ­£åœ¨åˆ›å»º 'actor_metadata' è¡¨...")
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS actor_metadata (
                        tmdb_id INTEGER PRIMARY KEY, 
                        profile_path TEXT, 
                        gender INTEGER, 
                        adult BOOLEAN,
                        popularity REAL, 
                        original_name TEXT, 
                        last_updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                        FOREIGN KEY(tmdb_id) REFERENCES person_identity_map(tmdb_person_id) ON DELETE CASCADE
                    )
                """)

                logger.trace("  -> æ­£åœ¨åˆ›å»º 'actor_subscriptions' è¡¨...")
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS actor_subscriptions (
                        id SERIAL PRIMARY KEY,
                        tmdb_person_id INTEGER NOT NULL UNIQUE,
                        actor_name TEXT NOT NULL,
                        profile_path TEXT,
                        config_start_year INTEGER DEFAULT 1900,
                        config_media_types TEXT DEFAULT 'Movie,TV',
                        config_genres_include_json JSONB,
                        config_genres_exclude_json JSONB,
                        status TEXT DEFAULT 'active',
                        last_checked_at TIMESTAMP WITH TIME ZONE,
                        added_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                        config_min_rating REAL DEFAULT 6.0
                    )
                """)
                cursor.execute("CREATE INDEX IF NOT EXISTS idx_as_status ON actor_subscriptions (status)")

                logger.trace("  -> æ­£åœ¨åˆ›å»º 'tracked_actor_media' è¡¨...")
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS tracked_actor_media (
                        id SERIAL PRIMARY KEY,
                        subscription_id INTEGER NOT NULL,
                        tmdb_media_id INTEGER NOT NULL,
                        media_type TEXT NOT NULL,
                        title TEXT NOT NULL,
                        release_date DATE,
                        poster_path TEXT,
                        status TEXT NOT NULL,
                        emby_item_id TEXT,
                        last_updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                        FOREIGN KEY(subscription_id) REFERENCES actor_subscriptions(id) ON DELETE CASCADE,
                        UNIQUE(subscription_id, tmdb_media_id)
                    )
                """)
                cursor.execute("CREATE INDEX IF NOT EXISTS idx_tam_subscription_id ON tracked_actor_media (subscription_id)")
                cursor.execute("CREATE INDEX IF NOT EXISTS idx_tam_status ON tracked_actor_media (status)")

                try:
                    logger.warning("  -> [æ•°æ®åº“å‡çº§] æ­£åœ¨åˆ é™¤æ—§çš„ 'resubscribe_settings' è¡¨...")
                    cursor.execute("DROP TABLE IF EXISTS resubscribe_settings CASCADE;")
                    logger.info("  -> [æ•°æ®åº“å‡çº§] æ—§è¡¨ 'resubscribe_settings' å·²æˆåŠŸåˆ é™¤ã€‚")
                except Exception as e_drop:
                    logger.error(f"  -> [æ•°æ®åº“å‡çº§] åˆ é™¤æ—§è¡¨ 'resubscribe_settings' æ—¶å‡ºé”™ï¼ˆå¯èƒ½å·²ä¸å­˜åœ¨ï¼‰: {e_drop}")
                
                logger.trace("  -> æ­£åœ¨åˆ›å»º 'resubscribe_rules' è¡¨ (å¤šè§„åˆ™æ´—ç‰ˆ)...")
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS resubscribe_rules (
                        id SERIAL PRIMARY KEY,
                        name TEXT NOT NULL UNIQUE,
                        enabled BOOLEAN DEFAULT TRUE,
                        
                        -- â˜… æ–°å¢ï¼šè§„åˆ™åº”ç”¨çš„ç›®æ ‡åª’ä½“åº“IDåˆ—è¡¨
                        target_library_ids JSONB, 
                        
                        -- â˜… æ–°å¢ï¼šæ´—ç‰ˆæˆåŠŸåæ˜¯å¦åˆ é™¤Embyåª’ä½“é¡¹
                        delete_after_resubscribe BOOLEAN DEFAULT FALSE,
                        
                        -- â˜… æ–°å¢ï¼šè§„åˆ™ä¼˜å…ˆçº§ï¼Œæ•°å­—è¶Šå°è¶Šä¼˜å…ˆ
                        sort_order INTEGER DEFAULT 0,

                        -- â–¼ ä¸‹é¢æ˜¯åŸæ¥ settings è¡¨é‡Œçš„æ‰€æœ‰å­—æ®µ
                        resubscribe_resolution_enabled BOOLEAN DEFAULT FALSE,
                        resubscribe_resolution_threshold INT DEFAULT 1920,
                        resubscribe_audio_enabled BOOLEAN DEFAULT FALSE,
                        resubscribe_audio_missing_languages JSONB,
                        resubscribe_subtitle_enabled BOOLEAN DEFAULT FALSE,
                        resubscribe_subtitle_missing_languages JSONB,
                        resubscribe_quality_enabled BOOLEAN DEFAULT FALSE,
                        resubscribe_quality_include JSONB,
                        resubscribe_effect_enabled BOOLEAN DEFAULT FALSE,
                        resubscribe_effect_include JSONB
                    )
                """)

                logger.trace("  -> æ­£åœ¨åˆ›å»º 'resubscribe_cache' è¡¨...")
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS resubscribe_cache (
                        item_id TEXT PRIMARY KEY,
                        item_name TEXT,
                        tmdb_id TEXT,
                        item_type TEXT,
                        status TEXT DEFAULT 'unknown', -- æ–°å¢çŠ¶æ€å­—æ®µ: 'ok', 'needed', 'subscribed'
                        reason TEXT,
                        resolution_display TEXT,
                        quality_display TEXT,
                        effect_display TEXT,
                        audio_display TEXT,
                        subtitle_display TEXT,
                        audio_languages_raw JSONB,
                        subtitle_languages_raw JSONB,
                        last_checked_at TIMESTAMP WITH TIME ZONE,
                        source_library_id TEXT
                    )
                """)
                cursor.execute("CREATE INDEX IF NOT EXISTS idx_resubscribe_cache_status ON resubscribe_cache (status);")

                # --- 2. æ‰§è¡Œå¹³æ»‘å‡çº§æ£€æŸ¥ ---
                logger.info("  -> å¼€å§‹æ‰§è¡Œæ•°æ®åº“è¡¨ç»“æ„å¹³æ»‘å‡çº§æ£€æŸ¥...")
                try:
                    # --- 2.1 æ£€æŸ¥æ‰€æœ‰è¡¨çš„åˆ— ---
                    # æŸ¥è¯¢ information_schema è·å–æ‰€æœ‰è¡¨çš„åˆ—ä¿¡æ¯
                    cursor.execute("""
                        SELECT table_name, column_name
                        FROM information_schema.columns
                        WHERE table_schema = current_schema();
                    """)
                    
                    # å°†ç»“æœç»„ç»‡æˆä¸€ä¸ªå­—å…¸ï¼Œæ–¹ä¾¿æŸ¥è¯¢: {'table_name': {'col1', 'col2'}, ...}
                    all_existing_columns = {}
                    for row in cursor.fetchall():
                        table = row['table_name']
                        if table not in all_existing_columns:
                            all_existing_columns[table] = set()
                        all_existing_columns[table].add(row['column_name'])

                    # --- 2.2 å®šä¹‰æ‰€æœ‰éœ€è¦æ£€æŸ¥å’Œæ·»åŠ çš„æ–°åˆ— ---
                    # æ ¼å¼: {'table_name': {'column_name': 'COLUMN_TYPE'}}
                    schema_upgrades = {
                        'media_metadata': {
                            "official_rating": "TEXT",
                            "unified_rating": "TEXT"
                        },
                        'watchlist': {
                            "last_episode_to_air_json": "JSONB"
                        },
                        'resubscribe_cache': {
                            "matched_rule_id": "INTEGER",
                            "matched_rule_name": "TEXT",
                            "source_library_id": "TEXT"
                        }
                    }

                    # --- 2.3 éå†å¹¶æ‰§è¡Œå‡çº§ ---
                    for table, columns_to_add in schema_upgrades.items():
                        # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨äºæˆ‘ä»¬æŸ¥è¯¢åˆ°çš„ä¿¡æ¯ä¸­
                        if table in all_existing_columns:
                            existing_cols_for_table = all_existing_columns[table]
                            for col_name, col_type in columns_to_add.items():
                                # å¦‚æœæ–°åˆ—ä¸å­˜åœ¨ï¼Œåˆ™æ·»åŠ å®ƒ
                                if col_name not in existing_cols_for_table:
                                    logger.info(f"    -> [æ•°æ®åº“å‡çº§] æ£€æµ‹åˆ° '{table}' è¡¨ç¼ºå°‘ '{col_name}' å­—æ®µï¼Œæ­£åœ¨æ·»åŠ ...")
                                    # ä½¿ç”¨ ALTER TABLE ... ADD COLUMN ... IF NOT EXISTS è¯­æ³•ï¼ŒåŒé‡ä¿é™©
                                    cursor.execute(f"ALTER TABLE {table} ADD COLUMN IF NOT EXISTS {col_name} {col_type};")
                                    logger.info(f"    -> [æ•°æ®åº“å‡çº§] å­—æ®µ '{col_name}' æ·»åŠ æˆåŠŸã€‚")
                                else:
                                    logger.trace(f"    -> å­—æ®µ '{table}.{col_name}' å·²å­˜åœ¨ï¼Œè·³è¿‡ã€‚")
                        else:
                            # è¿™ç§æƒ…å†µç†è®ºä¸Šä¸ä¼šå‘ç”Ÿï¼Œå› ä¸ºå‰é¢çš„ CREATE TABLE IF NOT EXISTS å·²ç»ä¿è¯äº†è¡¨çš„å­˜åœ¨
                            logger.warning(f"    -> [æ•°æ®åº“å‡çº§] æ£€æŸ¥è¡¨ '{table}' æ—¶å‘ç°è¯¥è¡¨ä¸å­˜åœ¨ï¼Œè·³è¿‡å‡çº§ã€‚")

                except Exception as e_alter:
                    logger.error(f"  -> [æ•°æ®åº“å‡çº§] æ£€æŸ¥æˆ–æ·»åŠ æ–°å­—æ®µæ—¶å‡ºé”™: {e_alter}", exc_info=True)
                    # å³ä½¿å‡çº§å¤±è´¥ï¼Œä¹Ÿç»§ç»­æ‰§è¡Œï¼Œä¸ä¸­æ–­ä¸»ç¨‹åºå¯åŠ¨
                
                try:
                    # æ£€æŸ¥ resubscribe_cache è¡¨ä¸Šæ˜¯å¦å·²å­˜åœ¨åä¸º fk_matched_rule çš„å¤–é”®
                    cursor.execute("""
                        SELECT 1 FROM pg_constraint 
                        WHERE conname = 'fk_matched_rule' AND conrelid = 'resubscribe_cache'::regclass;
                    """)
                    if cursor.fetchone() is None:
                        logger.info("    -> [æ•°æ®åº“å‡çº§] æ£€æµ‹åˆ° 'resubscribe_cache' è¡¨ç¼ºå°‘å¤–é”®ï¼Œæ­£åœ¨æ·»åŠ ...")
                        # ON DELETE SET NULL: å¦‚æœè§„åˆ™è¢«åˆ é™¤ï¼Œç¼“å­˜é¡¹çš„ matched_rule_id ä¼šè¢«è®¾ä¸º NULLï¼Œè€Œä¸æ˜¯åˆ é™¤ç¼“å­˜é¡¹
                        cursor.execute("""
                            ALTER TABLE resubscribe_cache 
                            ADD CONSTRAINT fk_matched_rule 
                            FOREIGN KEY (matched_rule_id) 
                            REFERENCES resubscribe_rules(id) 
                            ON DELETE SET NULL;
                        """)
                        logger.info("    -> [æ•°æ®åº“å‡çº§] å¤–é”® 'fk_matched_rule' æ·»åŠ æˆåŠŸã€‚")
                    else:
                        logger.trace("    -> å¤–é”® 'fk_matched_rule' å·²å­˜åœ¨ï¼Œè·³è¿‡ã€‚")
                except Exception as e_fk:
                     logger.error(f"  -> [æ•°æ®åº“å‡çº§] æ£€æŸ¥æˆ–æ·»åŠ å¤–é”®æ—¶å‡ºé”™: {e_fk}", exc_info=True)

                logger.info("  -> æ•°æ®åº“å¹³æ»‘å‡çº§æ£€æŸ¥å®Œæˆã€‚")

            conn.commit()
            logger.info("âœ… PostgreSQL æ•°æ®åº“åˆå§‹åŒ–å®Œæˆï¼Œæ‰€æœ‰è¡¨ç»“æ„å·²åˆ›å»º/éªŒè¯ã€‚")

    except psycopg2.Error as e_pg:
        logger.error(f"æ•°æ®åº“åˆå§‹åŒ–æ—¶å‘ç”Ÿ PostgreSQL é”™è¯¯: {e_pg}", exc_info=True)
        raise
    except Exception as e_global:
        logger.error(f"æ•°æ®åº“åˆå§‹åŒ–æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e_global}", exc_info=True)
        raise

# --- ä¿å­˜é…ç½®å¹¶é‡æ–°åŠ è½½çš„å‡½æ•° ---
def save_config_and_reload(new_config: Dict[str, Any]):
    """
    ã€æ–°ç‰ˆã€‘è°ƒç”¨é…ç½®ç®¡ç†å™¨ä¿å­˜é…ç½®ï¼Œå¹¶åœ¨æ­¤å¤„æ‰§è¡Œæ‰€æœ‰å¿…è¦çš„é‡æ–°åˆå§‹åŒ–æ“ä½œã€‚
    """
    try:
        # æ­¥éª¤ 1: è°ƒç”¨ config_manager æ¥ä¿å­˜æ–‡ä»¶å’Œæ›´æ–°å†…å­˜ä¸­çš„ config_manager.APP_CONFIG
        config_manager.save_config(new_config)
        
        # æ­¥éª¤ 2: æ‰§è¡Œæ‰€æœ‰ä¾èµ–äºæ–°é…ç½®çš„é‡æ–°åˆå§‹åŒ–é€»è¾‘
        initialize_processors()
        init_auth_from_blueprint()
        
        scheduler_manager.update_task_chain_job()
        
        logger.info("æ‰€æœ‰ç»„ä»¶å·²æ ¹æ®æ–°é…ç½®é‡æ–°åˆå§‹åŒ–å®Œæ¯•ã€‚")
        
    except Exception as e:
        logger.error(f"ä¿å­˜é…ç½®æ–‡ä»¶æˆ–é‡æ–°åˆå§‹åŒ–æ—¶å¤±è´¥: {e}", exc_info=True)
        # å‘ä¸ŠæŠ›å‡ºå¼‚å¸¸ï¼Œè®© API ç«¯ç‚¹å¯ä»¥æ•è·å®ƒå¹¶è¿”å›é”™è¯¯ä¿¡æ¯
        raise

# --- åˆå§‹åŒ–æ‰€æœ‰éœ€è¦çš„å¤„ç†å™¨å®ä¾‹ ---
def initialize_processors():
    """åˆå§‹åŒ–æ‰€æœ‰å¤„ç†å™¨ï¼Œå¹¶å°†å®ä¾‹èµ‹å€¼ç»™ extensions æ¨¡å—ä¸­çš„å…¨å±€å˜é‡ã€‚"""
    if not config_manager.APP_CONFIG:
        logger.error("æ— æ³•åˆå§‹åŒ–å¤„ç†å™¨ï¼šå…¨å±€é…ç½® APP_CONFIG ä¸ºç©ºã€‚")
        return

    current_config = config_manager.APP_CONFIG.copy()

    # --- 1. åˆ›å»ºå®ä¾‹å¹¶å­˜å‚¨åœ¨å±€éƒ¨å˜é‡ä¸­ ---
    
    # åˆå§‹åŒ– server_id_local
    server_id_local = None
    emby_url = current_config.get("emby_server_url")
    emby_key = current_config.get("emby_api_key")
    if emby_url and emby_key:
        server_info = emby_handler.get_emby_server_info(emby_url, emby_key)
        if server_info and server_info.get("Id"):
            server_id_local = server_info.get("Id")
            logger.trace(f"æˆåŠŸè·å–åˆ° Emby Server ID: {server_id_local}")
        else:
            logger.warning("æœªèƒ½è·å–åˆ° Emby Server IDï¼Œè·³è½¬é“¾æ¥å¯èƒ½ä¸å®Œæ•´ã€‚")

    # åˆå§‹åŒ– media_processor_instance_local
    try:
        media_processor_instance_local = MediaProcessor(config=current_config)
        logger.info("æ ¸å¿ƒå¤„ç†å™¨ å®ä¾‹å·²åˆ›å»º/æ›´æ–°ã€‚")
    except Exception as e:
        logger.error(f"åˆ›å»º MediaProcessor å®ä¾‹å¤±è´¥: {e}", exc_info=True)
        media_processor_instance_local = None

    # åˆå§‹åŒ– watchlist_processor_instance_local
    try:
        watchlist_processor_instance_local = WatchlistProcessor(config=current_config)
        logger.trace("WatchlistProcessor å®ä¾‹å·²æˆåŠŸåˆå§‹åŒ–ã€‚")
    except Exception as e:
        logger.error(f"åˆ›å»º WatchlistProcessor å®ä¾‹å¤±è´¥: {e}", exc_info=True)
        watchlist_processor_instance_local = None

    # åˆå§‹åŒ– actor_subscription_processor_instance_local
    try:
        actor_subscription_processor_instance_local = ActorSubscriptionProcessor(config=current_config)
        logger.trace("ActorSubscriptionProcessor å®ä¾‹å·²æˆåŠŸåˆå§‹åŒ–ã€‚")
    except Exception as e:
        logger.error(f"åˆ›å»º ActorSubscriptionProcessor å®ä¾‹å¤±è´¥: {e}", exc_info=True)
        actor_subscription_processor_instance_local = None


    # --- âœ¨âœ¨âœ¨ ç®€åŒ–ä¸ºâ€œå•ä¸€èµ‹å€¼â€ âœ¨âœ¨âœ¨ ---
    # ç›´æ¥èµ‹å€¼ç»™ extensions æ¨¡å—çš„å…¨å±€å˜é‡
    extensions.media_processor_instance = media_processor_instance_local
    extensions.watchlist_processor_instance = watchlist_processor_instance_local
    extensions.actor_subscription_processor_instance = actor_subscription_processor_instance_local
    extensions.EMBY_SERVER_ID = server_id_local

# --- ç”ŸæˆNginxé…ç½® ---
def ensure_nginx_config():
    """
    ã€Jinja2 å®¹å™¨é›†æˆç‰ˆã€‘ä½¿ç”¨ Jinja2 æ¨¡æ¿å¼•æ“ï¼Œç”Ÿæˆä¾›å®¹å™¨å†… Nginx ä½¿ç”¨çš„é…ç½®æ–‡ä»¶ã€‚
    """
    logger.info("æ­£åœ¨ç”Ÿæˆ Nginx é…ç½®æ–‡ä»¶...")
    
    # â˜…â˜…â˜… æ ¸å¿ƒä¿®æ”¹ 1: é…ç½®æ–‡ä»¶è·¯å¾„æ”¹ä¸ºå®¹å™¨å†… Nginx çš„æ ‡å‡†è·¯å¾„ â˜…â˜…â˜…
    final_config_path = '/etc/nginx/conf.d/default.conf'
    template_dir = os.path.join(os.getcwd(), 'templates', 'nginx')
    template_filename = 'emby_proxy.conf.template'

    try:
        # 1. è®¾ç½® Jinja2 ç¯å¢ƒ
        env = Environment(loader=FileSystemLoader(template_dir))
        template = env.get_template(template_filename)

        # 2. ä» APP_CONFIG è·å–å€¼
        emby_url = config_manager.APP_CONFIG.get(constants.CONFIG_OPTION_EMBY_SERVER_URL, "")
        nginx_listen_port = config_manager.APP_CONFIG.get(constants.CONFIG_OPTION_PROXY_PORT, 8097)
        redirect_url = config_manager.APP_CONFIG.get(constants.CONFIG_OPTION_PROXY_302_REDIRECT_URL, "")

        # 3. å‡†å¤‡æ›¿æ¢å€¼
        emby_upstream = emby_url.replace("http://", "").replace("https://", "").rstrip('/')
        # â˜…â˜…â˜… æ ¸å¿ƒä¿®æ”¹ 2: Nginx å’Œ Python ä»£ç†åœ¨åŒä¸€å®¹å™¨å†…ï¼Œä½¿ç”¨ localhost é€šä¿¡ â˜…â˜…â˜…
        proxy_upstream = "127.0.0.1:7758" 
        redirect_upstream = redirect_url.replace("http://", "").replace("https://", "").rstrip('/')

        if not emby_upstream:
            logger.error("config.ini ä¸­æœªé…ç½® Emby æœåŠ¡å™¨åœ°å€ï¼Œæ— æ³•ç”Ÿæˆ Nginx é…ç½®ï¼")
            sys.exit(1) # ä¸¥é‡é”™è¯¯ï¼Œç›´æ¥é€€å‡º

        # 4. å¡«å……æ¨¡æ¿
        context = {
            'EMBY_UPSTREAM': emby_upstream,
            'PROXY_UPSTREAM': proxy_upstream,
            'NGINX_LISTEN_PORT': nginx_listen_port,
            'REDIRECT_UPSTREAM': redirect_upstream
        }
        final_config_content = template.render(context)

        # 5. å†™å…¥æœ€ç»ˆçš„é…ç½®æ–‡ä»¶
        with open(final_config_path, 'w', encoding='utf-8') as f:
            f.write(final_config_content)
        
        logger.info(f"âœ… Nginx é…ç½®æ–‡ä»¶å·²æˆåŠŸç”Ÿæˆäº: {final_config_path}")

    except Exception as e:
        logger.error(f"ç”Ÿæˆ Nginx é…ç½®æ–‡ä»¶æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
        sys.exit(1) # ä¸¥é‡é”™è¯¯ï¼Œç›´æ¥é€€å‡º

# --- æ£€æŸ¥å­—ä½“æ–‡ä»¶ ---
def ensure_cover_generator_fonts():
    """
    å¯åŠ¨æ—¶æ£€æŸ¥ cover_generator/fonts ç›®å½•ä¸‹æ˜¯å¦æœ‰æŒ‡å®šå­—ä½“æ–‡ä»¶ï¼Œ
    è‹¥ç¼ºå°‘åˆ™ä»é¡¹ç›®æ ¹ç›®å½•çš„ fonts ç›®å½•æ‹·è´è¿‡å»ã€‚
    """
    cover_fonts_dir = os.path.join(config_manager.PERSISTENT_DATA_PATH, 'cover_generator', 'fonts')
    project_fonts_dir = os.path.join(os.getcwd(), 'fonts')  # é¡¹ç›®æ ¹ç›®å½•fonts

    required_fonts = [
        "en_font.ttf",
        "en_font_multi_1.otf",
        "zh_font.ttf",
        "zh_font_multi_1.ttf",
    ]

    if not os.path.exists(cover_fonts_dir):
        os.makedirs(cover_fonts_dir, exist_ok=True)
        logger.trace(f"å·²åˆ›å»ºå­—ä½“ç›®å½•ï¼š{cover_fonts_dir}")

    for font_name in required_fonts:
        dest_path = os.path.join(cover_fonts_dir, font_name)
        if not os.path.isfile(dest_path):
            src_path = os.path.join(project_fonts_dir, font_name)
            if os.path.isfile(src_path):
                try:
                    shutil.copy2(src_path, dest_path)
                    logger.trace(f"å·²æ‹·è´ç¼ºå¤±å­—ä½“æ–‡ä»¶ {font_name} åˆ° {cover_fonts_dir}")
                except Exception as e:
                    logger.error(f"æ‹·è´å­—ä½“æ–‡ä»¶ {font_name} å¤±è´¥: {e}", exc_info=True)
            else:
                logger.warning(f"é¡¹ç›®æ ¹ç›®å½•ç¼ºå°‘å­—ä½“æ–‡ä»¶ {font_name}ï¼Œæ— æ³•æ‹·è´è‡³ {cover_fonts_dir}")


# --- åº”ç”¨é€€å‡ºå¤„ç† ---
def application_exit_handler():
    # global media_processor_instance, scheduler, task_worker_thread # ä¸å†éœ€è¦ scheduler
    global media_processor_instance, task_worker_thread # ä¿®æ­£åçš„
    logger.info("åº”ç”¨ç¨‹åºæ­£åœ¨é€€å‡º (atexit)ï¼Œæ‰§è¡Œæ¸…ç†æ“ä½œ...")

    # 1. ç«‹åˆ»é€šçŸ¥å½“å‰æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡åœæ­¢
    if extensions.media_processor_instance: # ä» extensions è·å–
        logger.info("æ­£åœ¨å‘é€åœæ­¢ä¿¡å·ç»™å½“å‰ä»»åŠ¡...")
        extensions.media_processor_instance.signal_stop()

    task_manager.clear_task_queue()
    task_manager.stop_task_worker()

    # 4. å…³é—­å…¶ä»–èµ„æº
    if extensions.media_processor_instance: # ä» extensions è·å–
        extensions.media_processor_instance.close()
    
    scheduler_manager.shutdown()
    
    logger.info("atexit æ¸…ç†æ“ä½œæ‰§è¡Œå®Œæ¯•ã€‚")
atexit.register(application_exit_handler)

# --- åä»£ç›‘æ§ ---
@app.route('/api/health')
def health_check():
    """ä¸€ä¸ªç®€å•çš„å¥åº·æ£€æŸ¥ç«¯ç‚¹ï¼Œç”¨äº Docker healthcheckã€‚"""
    return jsonify({"status": "ok"}), 200

# --- webhooké€šçŸ¥ä»»åŠ¡ ---
@app.route('/webhook/emby', methods=['POST'])
@extensions.processor_ready_required
def emby_webhook():
    data = request.json
    event_type = data.get("Event") if data else "æœªçŸ¥äº‹ä»¶"
    logger.info(f"æ”¶åˆ°Emby Webhook: {event_type}")

    # --- æ‰¹é‡å¤„ç†å‡½æ•°ï¼šå¤„ç†é˜Ÿåˆ—ä¸­çš„æ‰€æœ‰æ–°å¢/å…¥åº“äº‹ä»¶ (æ­¤å‡½æ•°ä¸å˜) ---
    def _process_batch_webhook_events():
        # ... (è¿™ä¸ªå‡½æ•°çš„å†…éƒ¨é€»è¾‘ä¿æŒåŸæ ·)
        global WEBHOOK_BATCH_DEBOUNCER
        with WEBHOOK_BATCH_LOCK:
            items_to_process = list(set(WEBHOOK_BATCH_QUEUE)) # å»é‡
            WEBHOOK_BATCH_QUEUE.clear()
            WEBHOOK_BATCH_DEBOUNCER = None # é‡ç½® debouncer

        if not items_to_process:
            logger.debug("æ‰¹é‡å¤„ç†é˜Ÿåˆ—ä¸ºç©ºï¼Œæ— éœ€å¤„ç†ã€‚")
            return

        logger.info(f"  -> å¼€å§‹æ‰¹é‡å¤„ç† {len(items_to_process)} ä¸ª Emby Webhook æ–°å¢/å…¥åº“äº‹ä»¶ã€‚")
        for item_id, item_name, item_type in items_to_process:
            logger.info(f"  -> æ‰¹é‡å¤„ç†ä¸­: '{item_name}'")
            try:
                id_to_process = item_id
                if item_type == "Episode":
                    series_id = emby_handler.get_series_id_from_child_id(
                        item_id, extensions.media_processor_instance.emby_url,
                        extensions.media_processor_instance.emby_api_key, extensions.media_processor_instance.emby_user_id, item_name=item_name
                    )
                    if not series_id:
                        logger.warning(f"  -> æ‰¹é‡å¤„ç†ä¸­ï¼Œå‰§é›† '{item_name}' æœªæ‰¾åˆ°æ‰€å±å‰§é›†ï¼Œè·³è¿‡ã€‚")
                        continue
                    id_to_process = series_id
                
                full_item_details = emby_handler.get_emby_item_details(
                    item_id=id_to_process, emby_server_url=extensions.media_processor_instance.emby_url,
                    emby_api_key=extensions.media_processor_instance.emby_api_key, user_id=extensions.media_processor_instance.emby_user_id
                )
                if not full_item_details:
                    logger.warning(f"  -> æ‰¹é‡å¤„ç†ä¸­ï¼Œæ— æ³•è·å– '{item_name}' çš„è¯¦æƒ…ï¼Œè·³è¿‡ã€‚")
                    continue
                
                final_item_name = full_item_details.get("Name", f"æœªçŸ¥(ID:{id_to_process})")
                if not full_item_details.get("ProviderIds", {}).get("Tmdb"):
                    logger.warning(f"  -> æ‰¹é‡å¤„ç†ä¸­ï¼Œ'{final_item_name}' ç¼ºå°‘ Tmdb IDï¼Œè·³è¿‡ã€‚")
                    continue
                
                task_manager.submit_task(
                    webhook_processing_task,
                    task_name=f"Webhookä»»åŠ¡: {final_item_name}",
                    processor_type='media',
                    item_id=id_to_process,
                    force_reprocess=True
                )
                logger.info(f"  -> å·²å°† '{final_item_name}' æ·»åŠ åˆ°ä»»åŠ¡é˜Ÿåˆ—è¿›è¡Œå¤„ç†ã€‚")

            except Exception as e:
                logger.error(f"  -> æ‰¹é‡å¤„ç† '{item_name}' æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        logger.info("  -> æ‰¹é‡å¤„ç† Webhookä»»åŠ¡ å·²æ·»åŠ åˆ°åå°ä»»åŠ¡é˜Ÿåˆ—ã€‚")

    # â˜…â˜…â˜… æ ¸å¿ƒæ–°å¢ï¼šè¿™æ˜¯é˜²æŠ–è®¡æ—¶å™¨åˆ°æœŸåï¼ŒçœŸæ­£æ‰§è¡Œä»»åŠ¡çš„å‡½æ•° â˜…â˜…â˜…
    def _trigger_update_tasks(item_id, item_name, update_description, sync_timestamp_iso):
        """
        åœ¨é˜²æŠ–å»¶è¿Ÿç»“æŸåï¼Œå°†å…ƒæ•°æ®å’Œèµ„æºåŒæ­¥ä»»åŠ¡æäº¤åˆ°é˜Ÿåˆ—ã€‚
        """
        logger.info(f"é˜²æŠ–è®¡æ—¶å™¨åˆ°æœŸï¼Œä¸º '{item_name}' (ID: {item_id}) åˆ›å»ºæœ€ç»ˆçš„åŒæ­¥ä»»åŠ¡ã€‚")
        
        # ä»»åŠ¡1: åŒæ­¥å…ƒæ•°æ®åˆ°æ•°æ®åº“ç¼“å­˜
        task_manager.submit_task(
            task_sync_metadata_cache,
            task_name=f"å…ƒæ•°æ®ç¼“å­˜åŒæ­¥: {item_name}",
            processor_type='media',
            item_id=item_id,
            item_name=item_name
        )

        # ä»»åŠ¡2: åŒæ­¥åª’ä½“é¡¹åˆ°è¦†ç›–ç¼“å­˜
        task_manager.submit_task(
            task_sync_assets,
            task_name=f"è¦†ç›–ç¼“å­˜å¤‡ä»½: {item_name}",
            processor_type='media',
            item_id=item_id,
            update_description=update_description,
            sync_timestamp_iso=sync_timestamp_iso
        )

    # --- Webhook äº‹ä»¶åˆ†å‘é€»è¾‘ ---
    trigger_events = ["item.add", "library.new", "library.deleted", "metadata.update", "image.update"]
    if event_type not in trigger_events:
        logger.info(f"Webhookäº‹ä»¶ '{event_type}' ä¸åœ¨è§¦å‘åˆ—è¡¨ {trigger_events} ä¸­ï¼Œå°†è¢«å¿½ç•¥ã€‚")
        return jsonify({"status": "event_ignored_not_in_trigger_list"}), 200

    item_from_webhook = data.get("Item", {}) if data else {}
    original_item_id = item_from_webhook.get("Id")
    original_item_name = item_from_webhook.get("Name", "æœªçŸ¥é¡¹ç›®")
    original_item_type = item_from_webhook.get("Type")
    
    trigger_types = ["Movie", "Series", "Episode"]
    if not (original_item_id and original_item_type in trigger_types):
        logger.debug(f"Webhookäº‹ä»¶ '{event_type}' (é¡¹ç›®: {original_item_name}, ç±»å‹: {original_item_type}) è¢«å¿½ç•¥ã€‚")
        return jsonify({"status": "event_ignored_no_id_or_wrong_type"}), 200

    # --- å¤„ç†åˆ é™¤äº‹ä»¶ (é€»è¾‘ä¸å˜) ---
    if event_type == "library.deleted":
        try:
            with get_central_db_connection() as conn:
                log_manager = LogDBManager()
                log_manager.remove_from_processed_log(conn.cursor(), original_item_id)
                conn.commit()
            return jsonify({"status": "processed_log_entry_removed", "item_id": original_item_id}), 200
        except Exception as e:
            return jsonify({"status": "error_processing_remove_event", "error": str(e)}), 500
    
    # --- å¤„ç†æ–°å¢/å…¥åº“äº‹ä»¶ (ä½¿ç”¨æ‰¹é‡å¤„ç†, é€»è¾‘ä¸å˜) ---
    if event_type in ["item.add", "library.new"]:
        global WEBHOOK_BATCH_DEBOUNCER
        with WEBHOOK_BATCH_LOCK:
            WEBHOOK_BATCH_QUEUE.append((original_item_id, original_item_name, original_item_type))
            logger.debug(f"Webhookäº‹ä»¶ '{event_type}' (é¡¹ç›®: {original_item_name}) å·²æ·»åŠ åˆ°æ‰¹é‡é˜Ÿåˆ—ã€‚å½“å‰é˜Ÿåˆ—å¤§å°: {len(WEBHOOK_BATCH_QUEUE)}")
            
            if WEBHOOK_BATCH_DEBOUNCER is None or WEBHOOK_BATCH_DEBOUNCER.ready():
                logger.info(f"å¯åŠ¨ Webhook æ‰¹é‡å¤„ç† debouncerï¼Œå°†åœ¨ {WEBHOOK_BATCH_DEBOUNCE_TIME} ç§’åæ‰§è¡Œã€‚")
                WEBHOOK_BATCH_DEBOUNCER = spawn_later(WEBHOOK_BATCH_DEBOUNCE_TIME, _process_batch_webhook_events)
            else:
                logger.debug("Webhook æ‰¹é‡å¤„ç† debouncer æ­£åœ¨è¿è¡Œä¸­ï¼Œäº‹ä»¶å·²åŠ å…¥é˜Ÿåˆ—ã€‚")
        
        return jsonify({"status": "added_to_batch_queue", "item_id": original_item_id}), 202

    # â˜…â˜…â˜… æ ¸å¿ƒä¿®æ”¹ï¼šå°† metadata.update å’Œ image.update çº³å…¥é˜²æŠ–æœºåˆ¶ â˜…â˜…â˜…
    if event_type in ["metadata.update", "image.update"]:
        if not config_manager.APP_CONFIG.get(constants.CONFIG_OPTION_LOCAL_DATA_PATH):
            logger.debug(f"Webhook '{event_type}' æ”¶åˆ°ï¼Œä½†æœªé…ç½®æœ¬åœ°æ•°æ®æºï¼Œå°†å¿½ç•¥ã€‚")
            return jsonify({"status": "event_ignored_no_local_data_path"}), 200

        # å‡†å¤‡é€šç”¨å‚æ•°
        update_description = data.get("UpdateInfo", {}).get("Description", "Webhook Update")
        webhook_received_at_iso = datetime.now(timezone.utc).isoformat()

        # å‘ä¸Šè¿½æº¯åˆ°å‰§é›†/ç”µå½±çš„ID
        id_to_process = original_item_id
        name_for_task = original_item_name
        
        if original_item_type == "Episode":
            series_id = emby_handler.get_series_id_from_child_id(
                original_item_id, extensions.media_processor_instance.emby_url,
                extensions.media_processor_instance.emby_api_key, extensions.media_processor_instance.emby_user_id, item_name=original_item_name
            )
            if not series_id:
                logger.warning(f"Webhook '{event_type}': å‰§é›† '{original_item_name}' æœªæ‰¾åˆ°æ‰€å±å‰§é›†ï¼Œè·³è¿‡ã€‚")
                return jsonify({"status": "event_ignored_episode_no_series_id"}), 200
            id_to_process = series_id
            
            full_series_details = emby_handler.get_emby_item_details(
                item_id=id_to_process, emby_server_url=extensions.media_processor_instance.emby_url,
                emby_api_key=extensions.media_processor_instance.emby_api_key, user_id=extensions.media_processor_instance.emby_user_id
            )
            if full_series_details:
                name_for_task = full_series_details.get("Name", f"æœªçŸ¥å‰§é›†(ID:{id_to_process})")

        # --- é˜²æŠ–é€»è¾‘æ ¸å¿ƒ ---
        with UPDATE_DEBOUNCE_LOCK:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ­£åœ¨ç­‰å¾…çš„è®¡æ—¶å™¨
            if id_to_process in UPDATE_DEBOUNCE_TIMERS:
                # å¦‚æœæœ‰ï¼Œå–æ¶ˆå®ƒ
                old_timer = UPDATE_DEBOUNCE_TIMERS[id_to_process]
                old_timer.kill() # gevent ä½¿ç”¨ kill() æ¥å–æ¶ˆ
                logger.debug(f"å·²ä¸º '{name_for_task}' å–æ¶ˆäº†æ—§çš„åŒæ­¥è®¡æ—¶å™¨ï¼Œå°†ä»¥æœ€æ–°äº‹ä»¶ä¸ºå‡†ã€‚")

            # åˆ›å»ºä¸€ä¸ªæ–°çš„è®¡æ—¶å™¨ï¼Œå»¶è¿Ÿæ‰§è¡ŒçœŸæ­£çš„ä»»åŠ¡æäº¤å‡½æ•°
            logger.info(f"ä¸º '{name_for_task}' è®¾ç½®äº† {UPDATE_DEBOUNCE_TIME} ç§’çš„åŒæ­¥å»¶è¿Ÿï¼Œä»¥åˆå¹¶è¿ç»­çš„æ›´æ–°äº‹ä»¶ã€‚")
            new_timer = spawn_later(
                UPDATE_DEBOUNCE_TIME,
                _trigger_update_tasks,
                item_id=id_to_process,
                item_name=name_for_task,
                update_description=update_description,
                sync_timestamp_iso=webhook_received_at_iso
            )
            # å­˜å‚¨æ–°çš„è®¡æ—¶å™¨
            UPDATE_DEBOUNCE_TIMERS[id_to_process] = new_timer

        return jsonify({"status": "update_task_debounced", "item_id": id_to_process}), 202

    return jsonify({"status": "event_unhandled"}), 500

# --- å…œåº•è·¯ç”±ï¼Œå¿…é¡»æ”¾æœ€å ---
@app.route('/', defaults={'path': ''})
@app.route('/<path:path>')
def serve(path):
    static_folder_path = app.static_folder 

    if path != "" and os.path.exists(os.path.join(static_folder_path, path)):
        return send_from_directory(static_folder_path, path)
    else:
        return send_from_directory(static_folder_path, 'index.html')
    
# +++ åœ¨åº”ç”¨å¯¹è±¡ä¸Šæ³¨å†Œæ‰€æœ‰è“å›¾ +++
app.register_blueprint(watchlist_bp)
app.register_blueprint(collections_bp)
app.register_blueprint(custom_collections_bp)
app.register_blueprint(actor_subscriptions_bp)
app.register_blueprint(logs_bp)
app.register_blueprint(db_admin_bp)
app.register_blueprint(system_bp)
app.register_blueprint(media_api_bp) 
app.register_blueprint(media_proxy_bp)
app.register_blueprint(auth_bp)
app.register_blueprint(actions_bp)
app.register_blueprint(cover_generator_config_bp)
app.register_blueprint(tasks_bp)
app.register_blueprint(resubscribe_bp)

def main_app_start():
    """å°†ä¸»åº”ç”¨å¯åŠ¨é€»è¾‘å°è£…æˆä¸€ä¸ªå‡½æ•°"""
    from gevent.pywsgi import WSGIServer
    from geventwebsocket.handler import WebSocketHandler
    import gevent

    logger.info(f"åº”ç”¨ç¨‹åºå¯åŠ¨... ç‰ˆæœ¬: {constants.APP_VERSION}")
    
    config_manager.load_config()
    
    config_manager.LOG_DIRECTORY = os.path.join(config_manager.PERSISTENT_DATA_PATH, 'logs')
    try:
        log_size = int(config_manager.APP_CONFIG.get(constants.CONFIG_OPTION_LOG_ROTATION_SIZE_MB, constants.DEFAULT_LOG_ROTATION_SIZE_MB))
        log_backups = int(config_manager.APP_CONFIG.get(constants.CONFIG_OPTION_LOG_ROTATION_BACKUPS, constants.DEFAULT_LOG_ROTATION_BACKUPS))
    except (ValueError, TypeError):
        log_size = constants.DEFAULT_LOG_ROTATION_SIZE_MB
        log_backups = constants.DEFAULT_LOG_ROTATION_BACKUPS
    add_file_handler(log_directory=config_manager.LOG_DIRECTORY, log_size_mb=log_size, log_backups=log_backups)
    
    init_db()

    ensure_cover_generator_fonts()
    init_auth_from_blueprint()
    initialize_processors()
    task_manager.start_task_worker_if_not_running()
    scheduler_manager.start()
    
    def run_proxy_server():
        if config_manager.APP_CONFIG.get(constants.CONFIG_OPTION_PROXY_ENABLED):
            try:
                internal_proxy_port = 7758
                logger.trace(f"ğŸš€ [GEVENT] åå‘ä»£ç†æœåŠ¡å³å°†å¯åŠ¨ï¼Œç›‘å¬å†…éƒ¨ç«¯å£: {internal_proxy_port}")
                proxy_server = WSGIServer(('0.0.0.0', internal_proxy_port), proxy_app, handler_class=WebSocketHandler)
                proxy_server.serve_forever()
            except Exception as e:
                logger.error(f"å¯åŠ¨åå‘ä»£ç†æœåŠ¡å¤±è´¥: {e}", exc_info=True)
        else:
            logger.info("åå‘ä»£ç†åŠŸèƒ½æœªåœ¨é…ç½®ä¸­å¯ç”¨ã€‚")

    gevent.spawn(run_proxy_server)

    main_app_port = int(constants.WEB_APP_PORT)
    logger.info(f"ğŸš€ [GEVENT] ä¸»åº”ç”¨æœåŠ¡å™¨å³å°†å¯åŠ¨ï¼Œç›‘å¬ç«¯å£: {main_app_port}")
    
    class NullLogger:
        def write(self, data): pass
        def flush(self): pass

    main_server = WSGIServer(('0.0.0.0', main_app_port), app, log=NullLogger())
    main_server.serve_forever()

# â˜…â˜…â˜… æ ¸å¿ƒä¿®æ”¹ 2: æ–°å¢çš„å¯åŠ¨é€»è¾‘ï¼Œç”¨äºå¤„ç†å‘½ä»¤è¡Œå‚æ•° â˜…â˜…â˜…
if __name__ == '__main__':
    # æ£€æŸ¥æ˜¯å¦ä» entrypoint.sh ä¼ å…¥äº† 'generate-nginx-config' å‚æ•°
    if len(sys.argv) > 1 and sys.argv[1] == 'generate-nginx-config':
        print("Initializing to generate Nginx config...")
        # åªéœ€è¦åŠ è½½é…ç½®å’Œæ—¥å¿—ï¼Œç„¶åç”Ÿæˆå³å¯
        config_manager.load_config()
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨ï¼Œé¿å…æŠ¥é”™
        log_dir = os.path.join(config_manager.PERSISTENT_DATA_PATH, 'logs')
        os.makedirs(log_dir, exist_ok=True)
        add_file_handler(log_directory=log_dir)
        
        ensure_nginx_config()
        print("Nginx config generated successfully.")
        sys.exit(0) # æ‰§è¡Œå®Œæ¯•åæ­£å¸¸é€€å‡º
    else:
        # å¦‚æœæ²¡æœ‰ç‰¹æ®Šå‚æ•°ï¼Œåˆ™æ­£å¸¸å¯åŠ¨æ•´ä¸ªåº”ç”¨
        main_app_start()

# # --- ä¸»ç¨‹åºå…¥å£ç»“æŸ ---
